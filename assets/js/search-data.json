{
  
    
        "post0": {
            "title": "Kaggle Titanic dataset analysis",
            "content": ". Importing libraries . import pandas as pd import numpy as np import random as rnd import pylab as plot . !kaggle competitions download titanic -p ~/workspace/sudheer-blog/datasets/inputs/titanic/ . Downloading titanic.zip to /Users/sudheer/workspace/sudheer-blog/datasets/inputs/titanic 0%| | 0.00/34.1k [00:00&lt;?, ?B/s] 100%|██████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 7.00MB/s] . !unzip ~/workspace/sudheer-blog/datasets/inputs/titanic/titanic.zip -d ~/workspace/sudheer-blog/datasets/inputs/titanic &amp;&amp; rm ~/workspace/sudheer-blog/datasets/inputs/titanic/titanic.zip . Archive: /Users/sudheer/workspace/sudheer-blog/datasets/inputs/titanic/titanic.zip replace /Users/sudheer/workspace/sudheer-blog/datasets/inputs/titanic/gender_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C . import datasets . train_df = pd.read_csv(&#39;~/workspace/sudheer-blog/datasets/inputs/titanic/train.csv&#39;) test_df = pd.read_csv(&#39;~/workspace/sudheer-blog/datasets/inputs/titanic/test.csv&#39;) combine = [train_df, test_df] . Preview the datasets . train_df.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . test_df.head() . PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 892 | 3 | Kelly, Mr. James | male | 34.5 | 0 | 0 | 330911 | 7.8292 | NaN | Q | . 1 893 | 3 | Wilkes, Mrs. James (Ellen Needs) | female | 47.0 | 1 | 0 | 363272 | 7.0000 | NaN | S | . 2 894 | 2 | Myles, Mr. Thomas Francis | male | 62.0 | 0 | 0 | 240276 | 9.6875 | NaN | Q | . 3 895 | 3 | Wirz, Mr. Albert | male | 27.0 | 0 | 0 | 315154 | 8.6625 | NaN | S | . 4 896 | 3 | Hirvonen, Mrs. Alexander (Helga E Lindqvist) | female | 22.0 | 1 | 1 | 3101298 | 12.2875 | NaN | S | . Check the quality of the data . train_df.isnull().sum() . PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 . Both Age and Cabin has null lot of null values, lets see how much percentage of these are null . Check Null values percentage . train_df.isnull().sum(axis = 0)/train_df.count() . PassengerId 0.000000 Survived 0.000000 Pclass 0.000000 Name 0.000000 Sex 0.000000 Age 0.247899 SibSp 0.000000 Parch 0.000000 Ticket 0.000000 Fare 0.000000 Cabin 3.367647 Embarked 0.002250 Died 0.000000 dtype: float64 . Check Age distribution . train_df[&#39;Age&#39;].plot.hist(bins=12, alpha=0.5) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x115189710&gt; . Analyze by pivoting features . Lets analyse by following features . Age | Pclass | Sex | . Group by Age . sample = train_df sample[&#39;Died&#39;] = 1 - sample[&#39;Survived&#39;] sample.groupby(&#39;Sex&#39;).agg(&#39;sum&#39;)[[&#39;Survived&#39;,&#39;Died&#39;]].plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x114e36550&gt; . Group by Pclass . train_df[[&#39;Pclass&#39;, &#39;Survived&#39;]].groupby([&#39;Pclass&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Pclass Survived . 0 1 | 0.629630 | . 1 2 | 0.472826 | . 2 3 | 0.242363 | . Group by Sex . train_df[[&quot;Sex&quot;, &quot;Survived&quot;]].groupby([&#39;Sex&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Sex Survived . 0 female | 0.742038 | . 1 male | 0.188908 | .",
            "url": "https://sudheer.github.io/notebooks/2020/01/17/titanic_kaggle.html",
            "relUrl": "/notebooks/2020/01/17/titanic_kaggle.html",
            "date": " • Jan 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Functional programming in scala",
            "content": "Introduction . Functions are first class values Functions can be defined with in other functions | Functions can be passed as params to other functions | Functions can be return types of functions | . | Immutability operations on objects creates new objects rather than modifying original | . | Pure functionsPure Function = O/p depends on I/p + No side effects . A pure function cannot rely on input from files, databases, web services etc | always produces same output with given input | . | . pure functiondef add(a:Int, b:Int):Int = { return a + b } . | impure functiondef getSysTime:Long = { return System.currentTimeMillis() } . | . Defining functions . def functionName(params : paramTypes) : functionReturnType = { // statements } . def sqrt(x: Double) = x * x . defined function sqrt . def area(a: Int = 0, b: Int = 0) = { a * b } area(2,3) area(a=2) . defined function area res5_1: Int = 6 res5_2: Int = 0 . Function evaluation strategies . def test() = { println(&quot;testing call by value/name&quot;) 1 // return value } . defined function test . call by value . def callByValue(x: Int) = { println(&quot;x1=&quot; + x) println(&quot;x2=&quot; + x) } callByValue(test()) . testing call by value/name x1=1 x2=1 . defined function callByValue . call by name . def callByName(x: =&gt; Int) = { println(&quot;x1=&quot; + x) println(&quot;x2=&quot; + x) } callByName(test()) . testing call by value/name x1=1 testing call by value/name x2=1 . defined function callByName . def vs val for functions . val add = (a: Int, b: Int) =&gt; a + b add(2,3) . def add(a: Int, b: Int) =&gt; a + b add(2,3) . Blocks in scala . println({ val a = 2 + 2 a + 1 }) . 5 . variable definitions in blocks . val varName:String= { val d = &quot;Hello World&quot; d.toString() } . varName: String = &#34;Hello World&#34; . def meth2():String = { val d = &quot;Hello World&quot; d.toString() } println(meth2) . Hello World . defined function meth2 . Higher order functions . take function as argument | return function as argument | . . def math(x: Double, y: Double, f: (Double, Double) =&gt; Double) : Double = f(x,y) math(10, 20, (a,b)=&gt;a+b) math(10,20, (a,b)=&gt;a-b) . defined function math res0_1: Double = 30.0 res0_2: Double = -10.0 . val list = List(1,2,3) list.map(x =&gt; x+1) // Higher order function passed to map . list: List[Int] = List(1, 2, 3) res10_1: List[Int] = List(2, 3, 4) . Partiallly applied Functions . function which may not be defined for all inputs | . Creating partial functions . val multiplyVal = (x: Int, y: Int) =&gt; x * y val multiplyCurried = multiplyVal.curried // The call to .curried transforms function value&#39;s type from (Int, Int) =&gt; Int to Int =&gt; (Int =&gt; Int) val partiallyAppliedFunc1 = multiplyCurried(2) val partiallyAppliedFunc2 = multiplyVal(2, _: Int) val res1 = partiallyAppliedFunc1(3) val res2 = partiallyAppliedFunc2(3) . multiplyVal: (Int, Int) =&gt; Int = ammonite.$sess.cmd27$Helper$$Lambda$2404/1209197431@12a200a0 multiplyCurried: Int =&gt; Int =&gt; Int = scala.Function2$$Lambda$2298/1567973346@219b84fc partiallyAppliedFunc1: Int =&gt; Int = scala.Function2$$Lambda$2308/1602704739@170ea369 partiallyAppliedFunc2: Int =&gt; Int = ammonite.$sess.cmd27$Helper$$Lambda$2405/294871968@3d7fc26c res1: Int = 6 res2: Int = 6 . Currying . . Pseudo Code . result = f(x)(y)(z) f1 = f(x) f2 = f1(y) result = f2(z) . def add(a: Int)(b: Int) = a + b val twoPlusOne = add(2)(1) val plusOneCounter = add(1) _ val result = plusOneCounter(2) println(s&quot;add(2)(1) output is ${twoPlusOne} and result is $result&quot;) . add(2)(1) output is 3 and result is 3 . defined function add twoPlusOne: Int = 3 plusOneCounter: Int =&gt; Int = ammonite.$sess.cmd15$Helper$$Lambda$2113/1951893763@55ae6ff8 result: Int = 3 . def add(a: Int, b: Int) = a + b val addCurried = add(2, _) sumCurried(6) . defined function sum sumCurried: Int =&gt; Int = ammonite.$sess.cmd32$Helper$$Lambda$2512/507796500@f24aa23 res32_2: Int = 8 . Functional Composition . val add1 = (a: Int) =&gt; a+1 val multiply3 = (a: Int) =&gt; a*3 . add1: Int =&gt; Int = ammonite.$sess.cmd1$Helper$$Lambda$1829/1144892518@2e7ed391 multiply3: Int =&gt; Int = ammonite.$sess.cmd1$Helper$$Lambda$1830/1298967826@3f84a3ec . // AndThen val andThenExample = add1 andThen multiply3 andThenExample(3) . andTahenExample: Int =&gt; Int = scala.Function1$$Lambda$320/899929247@27071089 res6_1: Int = 12 . // compose val composeExample = add1 compose multiply3 composeExample (1) . composeExample: Int =&gt; Int = scala.Function1$$Lambda$2020/929592323@3dec46e5 res9_1: Int = 4 . Declarative style . FP helps us to write declarative style of programming thus reducing the number of lines and defining a cleaner way. Given a list to modify, lets see both imperative vs declarative styles. . val x = List(10,20,30,40) . x: List[Int] = List(10, 20, 30, 40) . Imperative style . an apply method is generated | constructor params are public vals by default | equals and hashcode are generated | tostring method is generated | unapply method is generated | has built in copy method which is helpful in cloning | . import scala.collection.mutable.ListBuffer val mutable = new ListBuffer[Int] for (e &lt;- x) { mutable += (e * 3) } println(mutable.toList) . List(30, 60, 90, 120) . import scala.collection.mutable.ListBuffer mutable: ListBuffer[Int] = ListBuffer(30, 60, 90, 120) . Declarative style . val a = Seq({ val list = List(1,2,3,4) list.filter(x =&gt; x&gt;2) }) println(a) . List(List(3, 4)) . a: Seq[List[Int]] = List(List(3, 4)) . val result = x.map(element =&gt; element * 3) println(result) . List(30, 60, 90, 120) . result: List[Int] = List(30, 60, 90, 120) . Closures . Closures are functions whose return values depends on 1 or more values outside of function . var factor = 7 val multiplier = (i:Int) =&gt; i * factor multiplier(7) . factor: Int = 7 multiplier: Int =&gt; Int = ammonite.$sess.cmd31$Helper$$Lambda$2482/1360119030@25b7c499 res31_2: Int = 49 .",
            "url": "https://sudheer.github.io/programming/2019/05/12/fp_scala.html",
            "relUrl": "/programming/2019/05/12/fp_scala.html",
            "date": " • May 12, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "Shortcuts and Commands",
            "content": ". Jupyter . Cell . m : Switch to markdown y : Switch to code Ctrl + Shift + - :while editing a cell should split it at the cursor D + D (press the key twice): delete current cell z : Undo cell deletion . Execute . Shift + Enter : execute and go to next cell Ctrl + Enter : execute and live in current cell . Insert . a : To create cell above to current cell b : To create cell bellow to current cell . Cut Copy Paste . x : To cut the cell that can be pasted any where c : To copy the cell that can be pasted any where v : To paste the cell . Markdown in jupyter . #, ## , ###, #### : headings **string** : bold text $ $ : math symbols &gt; :indenting- : bullets 1 : numbered bullets . Unix commands . !&lt;command&gt; . Download kaggle datasets . !pip install -q kaggle . !kaggle competitions --help .",
            "url": "https://sudheer.github.io/installations/2018/12/03/shortcuts.html",
            "relUrl": "/installations/2018/12/03/shortcuts.html",
            "date": " • Dec 3, 2018"
        }
        
    
  
    
        ,"post3": {
            "title": "Pyspark quickstart",
            "content": "Creating RDD . # from list parallelRdd = sc.parallelize([1,2,3,4,5]) parallelRdd.collect() . &#9656; | : | . | . . [1, 2, 3, 4, 5] . # from tuple rdd = sc.parallelize((&#39;a&#39;,&#39;b&#39;,&#39;c&#39;)) rdd.collect() . &#9656; | : | . | . . [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] . # from list rdd = sc.parallelize([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;]) rdd.collect() . &#9656; | : | . | . . [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] . # from set rdd = sc.parallelize({&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;}) rdd.collect() . &#9656; | : | . | . . [&#39;c&#39;, &#39;d&#39;, &#39;b&#39;, &#39;a&#39;] . # from dict rdd = sc.parallelize( { &#39;a&#39; : 1, &#39;b&#39; : 2, &#39;c&#39; : 3 }) rdd.collect() . &#9656; | : | . | . . [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] . As you see only keys are used in the dictionary to form RDD . # read from csv file rdd = sc.textFile(&#39;&#39;) rdd.take(2) . # create empty rdd rdd = sc.emptyRDD() rdd.isEmpty() . &#9656; | : | . | . . True . # from namedtuple from collections import namedtuple Person = namedtuple(&quot;Person&quot;, &quot;id firstName lastName&quot;) jon = Person(1, &quot;Jon&quot;, &quot;Doe&quot;) jane = Person(2, &quot;Jane&quot;, &quot;Doe&quot;) rdd = sc.parallelize([jon, jane]) rdd.collect() . &#9656; | : | . | . . [Person(id=1, firstName=&#39;Jon&#39;, lastName=&#39;Doe&#39;), Person(id=2, firstName=&#39;Jane&#39;, lastName=&#39;Doe&#39;)] . Test Rdd Functions . # Histogram rdd = sc.parallelize([1,2,3,4,5]) rdd.histogram([0,10][1]) . &#9656; | : | . | . . ([1.0, 1.4, 1.8, 2.2, 2.6, 3.0, 3.4000000000000004, 3.8000000000000003, 4.2, 4.6, 5], [1, 0, 1, 0, 0, 1, 0, 1, 0, 1]) . # sum rdd.sum() . &#9656; | : | . | . . 15 . rdd.distinct().count() . &#9656; | : | . | . . 2 . # convert rdd to pair rdd using map sc.parallelize([1,2,3,4,5,6]).map(lambda x: (x%3, x)).collect() . &#9656; | : | . | . . [(1, 1), (2, 2), (0, 3), (1, 4), (2, 5), (0, 6)] . # take sc.parallelize([1,2,3,4,5,6]).first() . &#9656; | : | . | . . 1 . # sort by key sc.parallelize([1,6,3,4,2,7]).map(lambda x: (x%3, x)).sortByKey(ascending=ascending, numPartitions=5).collect() DataFrame&#39;s . create dataframe by reading csv file . csvData = spark.read.csv( path=&#39;&#39;, sep=&#39;,&#39;, encoding=&#39;UTF-8&#39;, comment=None, header=True, inferSchema=True) csvData.show(n=5, truncate=False) . create dataframe by createDataFrame function . rdd = spark.createDataFrame( [ (1, &#39;foo&#39;), (2, &#39;bar&#39;), ], [&#39;id&#39;, &#39;txt&#39;] # schema of columns here ) rdd.show(2) . &#9656; | : | . | . . +++ | id|txt| +++ | 1|foo| | 2|bar| +++ . change column names . Option-1 . from pyspark.sql.functions import col rdd.selectExpr(&quot;id as emp_id&quot;, &quot;txt as name&quot;).printSchema() . &#9656; | : | . | . . root |-- emp_id: long (nullable = true) |-- name: string (nullable = true) . Option-2 . from pyspark.sql.functions import col rdd.select(col(&quot;id&quot;).alias(&quot;emp_id&quot;), col(&quot;txt&quot;).alias(&quot;name&quot;)).printSchema() . &#9656; | : | . | . . root |-- emp_id: long (nullable = true) |-- name: string (nullable = true) . Option-3 . rdd.registerTempTable(&quot;rdd&quot;) output = spark.sql(&quot;SELECT id AS emp_id, txt as Name from rdd&quot;) output.printSchema() . &#9656; | : | . | . . root |-- emp_id: long (nullable = true) |-- Name: string (nullable = true) . Group by and Aggregate . rdd = spark.createDataFrame( [ (&#39;GOOG&#39;, 1, 200000), (&#39;GOOG&#39;, 2, 150000), (&#39;AAPL&#39;, 3, 175000), (&#39;AAPL&#39;, 4, 180000) ], [&#39;company&#39;, &#39;emp_id&#39;, &#39;salary&#39;] # schema of columns here ) rdd.show() . &#9656; | : | . | . . +-+++ |company|emp_id|salary| +-+++ | GOOG| 1|200000| | GOOG| 2|150000| | AAPL| 3|175000| | AAPL| 4|180000| +-+++ . rdd.groupBy(&#39;company&#39;) . &#9656; | : | . | . . &lt;pyspark.sql.group.GroupedData at 0x7f351e56d278&gt; . rdd.groupBy(&#39;company&#39;).max().show() ## you see if gave max values for a company for both columns . &#9656; | : | . | . . +-+--+--+ |company|max(emp_id)|max(salary)| +-+--+--+ | AAPL| 4| 180000| | GOOG| 2| 200000| +-+--+--+ . rdd.groupBy(&#39;company&#39;).max(&#39;salary&#39;).show() . &#9656; | : | . | . . +-+--+ |company|max(salary)| +-+--+ | AAPL| 180000| | GOOG| 200000| +-+--+ . # Sum by Aggregate grouped_rdd = rdd.groupBy(&quot;company&quot;) grouped_rdd.agg({&#39;salary&#39;:&#39;avg&#39;}).show() . &#9656; | : | . | . . +-+--+ |company|avg(salary)| +-+--+ | AAPL| 177500.0| | GOOG| 175000.0| +-+--+ . rdd.groupBy(&quot;company&quot;) .count() .orderBy(&quot;count&quot;, ascending=False) .show(5) . &#9656; | : | . | . . +-+--+ |company|count| +-+--+ | AAPL| 2| | GOOG| 2| +-+--+ . Order by . rdd.orderBy(&#39;salary&#39;).show() . &#9656; | : | . | . . +-+++ |company|emp_id|salary| +-+++ | GOOG| 2|150000| | AAPL| 3|175000| | AAPL| 4|180000| | GOOG| 1|200000| +-+++ . rdd.orderBy(rdd[&quot;salary&quot;].desc()).show() . &#9656; | : | . | . . +-+++ |company|emp_id|salary| +-+++ | GOOG| 1|200000| | AAPL| 4|180000| | AAPL| 3|175000| | GOOG| 2|150000| +-+++ . Adding &amp; Dropping Columns . from pyspark.sql.types import DoubleType addCol = rdd.withColumn(&quot;doubleSalary&quot;, rdd[&#39;salary&#39;].cast(DoubleType())) addCol.printSchema() . &#9656; | : | . | . . root |-- company: string (nullable = true) |-- emp_id: long (nullable = true) |-- salary: long (nullable = true) |-- doubleSalary: double (nullable = true) . addCol.drop(&quot;doubleSalary&quot;).printSchema() . &#9656; | : | . | . . root |-- company: string (nullable = true) |-- emp_id: long (nullable = true) |-- salary: long (nullable = true) . Joins .",
            "url": "https://sudheer.github.io/computing/2018/03/12/pyspark_quickstart.html",
            "relUrl": "/computing/2018/03/12/pyspark_quickstart.html",
            "date": " • Mar 12, 2018"
        }
        
    
  
    
        ,"post4": {
            "title": "Apach Spark Transformations",
            "content": ". Aggregate . The aggregate can return a different type than RDD on which we are working on. It allows users to apply 2 functions, one on top of each partition (input type T =&gt; U), other to aggregate the results of all the partitions into final result (merging 2 U&#39;s). Both the functions have to be commutative and associative. We can also specify a initial value. . val inputRdd = sc.parallelize(Array(1,2,3,4,5,6,7)) val output = inputRdd.aggregate(0)((x,y) =&gt; x+y, (u,v) =&gt; u+v); . inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:27 output: Int = 28 . cartesian . Return the Cartesian product of this RDD and another one. result contains all pairs of (a,b) where a belongs to rdd1 and b belongs to rdd2. . val rdd1 = sc.parallelize(1 to 2) val rdd2 = sc.parallelize(3 to 5) val output = rdd1.cartesian(rdd2).collect() . rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at &lt;console&gt;:25 rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at &lt;console&gt;:26 output: Array[(Int, Int)] = Array((1,3), (1,4), (1,5), (2,3), (2,4), (2,5)) . countByValue . Returns count of each unique value in this RDD as a local map of (value, count) pairs. Should be careful while using this when you have large data as it sends the results to driver. . val inputRdd = sc.parallelize(1 to 10) val output = inputRdd.countByValue() . inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at &lt;console&gt;:27 output: scala.collection.Map[Int,Long] = Map(5 -&gt; 1, 10 -&gt; 1, 1 -&gt; 1, 6 -&gt; 1, 9 -&gt; 1, 2 -&gt; 1, 7 -&gt; 1, 3 -&gt; 1, 8 -&gt; 1, 4 -&gt; 1) . collect . Used only when the rdd is small enough. Return an array that contains all of the elements in this RDD. . val output = sc.parallelize(1 to 10, 2).collect() . output: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) . distinct . Return a new RDD containing the distinct elements in this RDD. Shuffle happens on this transformation. . val data = Seq(1,1,2,2,3,4) val inputRdd = sc.parallelize(data) val output = inputRdd.distinct(); output.collect() . data: Seq[Int] = List(1, 1, 2, 2, 3, 4) inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:30 output: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at distinct at &lt;console&gt;:31 res0: Array[Int] = Array(1, 2, 3, 4) . filter . Return a new RDD containing only the elements that satisfy a predicate. . val data = Seq(1, 2, 3, 4) val inputRdd = sc.parallelize(data) val filterOutput = inputRdd.filter( s =&gt; s&gt;2 ) filterOutput.collect(); . data: Seq[Int] = List(1, 2, 3, 4) inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:29 filterOutput: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[15] at filter at &lt;console&gt;:30 res1: Array[Int] = Array(3, 4) . first . Return the first element in this RDD. . val output = sc.parallelize(1 to 10).first(); . output: Int = 1 . flatMap . Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. You need to supply (A ⇒ Iterable[B]) function to the flatMap i.e on each element of input A, Map is applied followed by flatten which is flatMap. . val data = Seq(1, 2, 3, 4) val inputRdd = sc.parallelize(data) val output = inputRdd.flatMap( s =&gt; List(s,s+1,s+2)); output.collect(); . data: Seq[Int] = List(1, 2, 3, 4) inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at &lt;console&gt;:31 output: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[18] at flatMap at &lt;console&gt;:32 res2: Array[Int] = Array(1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5, 6) . glom . Return an RDD created by coalescing all elements within each partition into an array. . val inputRdd = sc.parallelize(1 to 10, 2) val output = inputRdd.glom().collect(); . inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[19] at parallelize at &lt;console&gt;:27 output: Array[Array[Int]] = Array(Array(1, 2, 3, 4, 5), Array(6, 7, 8, 9, 10)) . groupBy . Return an RDD of grouped items. | Each group consists of a key and a sequence of elements mapping to that key. | Ordering is not guaranteed, not same for every execution. | Shuffle happens, better to use reduceby than groupby since it does not combine in each partition itself ( i.e groupBy happens in reduce phase), hence result high network traffic. | . val rdd1 = sc.parallelize(Array(2,3,4,1,3,4)) val output = rdd1.groupBy(x =&gt; x).collect() . rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[21] at parallelize at &lt;console&gt;:27 output: Array[(Int, Iterable[Int])] = Array((1,CompactBuffer(1)), (2,CompactBuffer(2)), (3,CompactBuffer(3, 3)), (4,CompactBuffer(4, 4))) . intersection . Return the intersection of this RDD and another one. | Result will not contain any duplicate. | Done by map, co-group, filter in the background. | performs a shuffle internally | . val output = sc.parallelize(Array(1,2,3,4)) .intersection(sc.parallelize(Array(3,4,5,6))) .collect() . output: Array[Int] = Array(3, 4) . map . Return a new RDD by applying a function to all elements of this RDD . val data = Seq(1, 2, 3, 4) val inputRdd = sc.parallelize(data) val output = inputRdd.map( s =&gt; s+1 ) // applying anonymus function to rdd elements. output.collect(); . data: Seq[Int] = List(1, 2, 3, 4) inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[32] at parallelize at &lt;console&gt;:32 output: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[33] at map at &lt;console&gt;:33 res3: Array[Int] = Array(2, 3, 4, 5) . mapPartitions . Return a new RDD by applying a function to each partition of this RDD. . val rdd1 = sc.parallelize(1 to 20,3) val output = rdd1.mapPartitions(x =&gt; x).collect(); . rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at &lt;console&gt;:27 output: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20) . mapPartitionsWithIndex . Similar to mapPartitions, additionally tracks the index of the original partition. . val rdd1 = sc.parallelize(1 to 20,3) val output = rdd1.mapPartitions(x =&gt; x).collect(); . rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[36] at parallelize at &lt;console&gt;:27 output: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20) . mapPartitionsWithIndex . Similar to mapPartitions, additionally tracks the index of the original partition. . val inputRdd = sc.parallelize(1 to 10, 2) val output = inputRdd.mapPartitionsWithIndex((idx, itr) =&gt; itr.map(s =&gt; (idx, s))).collect() . inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[38] at parallelize at &lt;console&gt;:27 output: Array[(Int, Int)] = Array((0,1), (0,2), (0,3), (0,4), (0,5), (1,6), (1,7), (1,8), (1,9), (1,10)) . randomSplit . Randomly splits this RDD with the provided weights. You can specify the fraction weights the output rdd&#39;s needs to split. However you can see they are not exactly equally split based on fraction as in example. . val inputRdd = sc.parallelize(1 to 10) val output = inputRdd.randomSplit(Array(0.5,0.5)) // return&#39;s array of rdd&#39;s output(0).collect() // rdd in 0th location output(1).collect() // rdd in 1st location . inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[40] at parallelize at &lt;console&gt;:28 output: Array[org.apache.spark.rdd.RDD[Int]] = Array(MapPartitionsRDD[41] at randomSplit at &lt;console&gt;:29, MapPartitionsRDD[42] at randomSplit at &lt;console&gt;:29) res4: Array[Int] = Array(2, 4, 7, 9) . reduce . Reduces the elements of this RDD. function in reduce obey&#39;s commutative and associative properties. . val output = sc.parallelize(1 to 5).reduce((u,v) =&gt; u + v) . output: Int = 15 . repartition . Return a new RDD that has exactly the passed argument partitions to this method. . val data = Seq(1, 2, 3, 4) val inputRdd = sc.parallelize(data,2) inputRdd.partitions; // Get the array of partitions of this RDD inputRdd.repartition(1) . data: Seq[Int] = List(1, 2, 3, 4) inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[44] at parallelize at &lt;console&gt;:30 res5: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[48] at repartition at &lt;console&gt;:34 . sample . Return a sampled subset of this RDD. . val inputRdd = sc.parallelize(1 to 10, 3) inputRdd.sample(true, 0.3, 0).collect() . inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[49] at parallelize at &lt;console&gt;:27 res6: Array[Int] = Array(9, 10) . sortBy . Return this RDD sorted by the given key function. You should pass function since its not pair rdd to generate key, boolean (asce/desc). . val output = sc.parallelize(Array(3,4,2,1)) .sortBy(x =&gt; x,false) // desc order by false .collect() . output: Array[Int] = Array(4, 3, 2, 1) . subtract . Subtracts elements of one rdd from other . val output = sc.parallelize(1 to 10).subtract(sc.parallelize(5 to 15)) output.collect() . output: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[62] at subtract at &lt;console&gt;:27 res7: Array[Int] = Array(1, 2, 3, 4) . take . Take the first num elements of the RDD. It works by first scanning one partition, and use the results from that partition to estimate the number of additional partitions needed to satisfy the limit. . val output = sc.parallelize(1 to 10).take(2) . output: Array[Int] = Array(1, 2) . takeSample . Return a fixed-size sampled subset of this RDD in an array. . sample takeSample . It returns an RDD | It returns an Array | . Return a fixed-size sampled subset | Return a fixed-size sampled subset | . Should specify sample as Double fraction arg | sample is specified as Int | . val inputRdd = sc.parallelize(1 to 10, 3) inputRdd.takeSample(true, 3, 0); . inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[64] at parallelize at &lt;console&gt;:27 res8: Array[Int] = Array(7, 7, 4) . toLocalIterator . Return an iterator by converting RDD into a scala iterator that contains all of the elements in this RDD. . val output = sc.parallelize(1 to 5, 1).toLocalIterator while (output.hasNext) { println(output.next) } . 1 2 3 4 5 . output: Iterator[Int] = empty iterator . union . Return the union of this RDD and another one. | Identical elements will appear multiple times. | Need to use distinct to eliminate them. | Can also use ++ instead of union. | . val a = sc.parallelize(1 to 10, 1) val b = sc.parallelize(10 to 20, 1) a.union(b).collect(); a.union(b).distinct().collect(); . a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[67] at parallelize at &lt;console&gt;:25 b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[68] at parallelize at &lt;console&gt;:26 res10: Array[Int] = Array(4, 16, 14, 6, 8, 12, 18, 20, 10, 2, 13, 19, 15, 11, 1, 17, 3, 7, 9, 5) .",
            "url": "https://sudheer.github.io/computing/2017/01/12/spark_transformations.html",
            "relUrl": "/computing/2017/01/12/spark_transformations.html",
            "date": " • Jan 12, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "TBD .",
          "url": "https://sudheer.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sudheer.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}