{
  
    
  
    
  
    
  
    
        "post3": {
            "title": "Statistics for Machine Learning",
            "content": ". Data types in stats . Examples of Numerical . # continous mu = 20 sigma=2 data_continous = numpy.random.normal(mu, sigma, 1000) # generate from 100 to 150 with 0.1 difference sns.distplot(data_continous, color=&quot;blue&quot;) plt.show() # discrete import numpy as np dice_rolls = [np.random.randint(1, 7) for _ in range(10)] plt.hist(dice_rolls) plt.show() . Nominal Data . Data you can&#39;t order. . Gender | religion | hair color | . data = {&#39;Name&#39;: [&#39;Jim&#39;,&#39;Jake&#39;,&#39;Jessy&#39;], &#39;Gender&#39;: [&#39;Male&#39;,&#39;Male&#39;,&#39;Female&#39;] } data = pd.DataFrame(data, columns=[&#39;Name&#39;, &#39;Gender&#39;]) data . Name Gender . 0 Jim | Male | . 1 Jake | Male | . 2 Jessy | Female | . Ordinal Data . Data you can order, but can&#39;t do arthematic. . customer ratings | economic status | . data = {&#39;Movie&#39;: [&#39;Superman&#39;,&#39;Heman&#39;,&#39;Spiderman&#39;], &#39;Rating&#39;: [4.0,4.7,4.9] } data = pd.DataFrame(data, columns=[&#39;Movie&#39;, &#39;Rating&#39;]) data . Movie Rating . 0 Superman | 4.0 | . 1 Heman | 4.7 | . 2 Spiderman | 4.9 | . Central Tendancy . Generate data . import numpy as np import pandas as pd from numpy.random import seed from numpy.random import randint from numpy import mean import seaborn as sns import matplotlib.pyplot as plt # lets generate some weights of population # seed the random number generator seed(1) # generate a sample of weights of population weights = randint(low=120, high=200, size=10000) . Lets plot the histogram of weights of population and analyse it. . import matplotlib.pyplot as plt sns.distplot(weights, color=&quot;blue&quot;) plt.xlabel(&quot;weights&quot;) plt.ylabel(&quot;frequency&quot;) plt.show() . Mean . $ mu= frac{ sum_{i=1}^{N} x_{i}}{N}$ . one of the several values to descrie central tendancy of the data | . import numpy numpy.mean(weights) . 159.6552 . Median . if n is odd . $Median = left( frac{n+1}{2} right)^{t h} term$ . if n is even . $ text { Median }= frac{ left( frac{n}{2} right)^{t h} text {term}+ left( frac{n}{2}+1 right)^{t h} text { term }}{2}$ . numpy.median(weights) . 159.0 . Mode . Mode is the most frequently occured value in our distribution. . from scipy import stats stats.mode(weights) . ModeResult(mode=array([152]), count=array([152])) . How they change . Measure of spread . Range . Range is the difference between min and max value. It shows how much our data is spread. . np.max(weights) - np.min(weights) . 79 . Quartiles . from numpy import percentile # calculate quartiles quartiles = percentile(weights, [25, 50, 75]) print(&#39;Q1: %.3f&#39; % quartiles[0]) print(&#39;Q2 or Median: %.3f&#39; % quartiles[1]) print(&#39;Q3: %.3f&#39; % quartiles[2]) print(&#39;Q4 or Max: %.3f&#39; % np.max(weights)) . Q1: 140.000 Q2 or Median: 159.000 Q3: 180.000 Q4 or Max: 199.000 . Variance . Measures how much data is spread from the mean. For technical reasons, we use (n-1) in denominator. . $ text { Variance }=s^{2}= frac{ sum(x_i- bar{x})^{2}}{n-1}$ . $(x_i - bar{x})$ is deviation from mean for every value of sample, so variance is mean squared deviation . . np.var(weights) . 537.16871296 . Std Deviation . Measures the spread from the mean. You can think of it like average distance of data from mean. To negate the squares applied earlier, we do square root here. . $s= sqrt{ frac{1}{n} sum_{i=1}^{n} left(x_{i}- bar{x} right)^{2}}$ . . For a normal distribution let&#39;s see how standard deviation varies from the mean . The below percentages are for most plots which are normally distributed. . . np.std(weights) . 23.1769004174415 . Covariance &amp; Co-relation . . Covariance . covariance measures how two variables are dependent on each other. For a positive covariance 2nd variable increases if 1st increases. For negetive one decreases while other increases. . $ operatorname{cov}(X, Y)= frac{ sum_{i=1}^{N} left(x_{i}- mu_{x} right) left(y_{i}- mu_{y} right)}{N}$ . Corelation . corelation co-efficient : value lies always between -1 and 1 . $ rho_{x, y}= frac{ operatorname{cov}(X, Y)}{ sigma_{x}, sigma_{y}}$ . . # Import pandas library import pandas as pd # initialize list of lists data = [[180, 160], [160, 175], [155, 125], [158, 148]] # Create the pandas DataFrame df = pd.DataFrame(data, columns = [&#39;Height&#39;, &#39;Weight&#39;]) # print dataframe. df . Height Weight . 0 180 | 160 | . 1 160 | 175 | . 2 155 | 125 | . 3 158 | 148 | . numpy.corrcoef(df[&#39;Height&#39;], df[&#39;Weight&#39;]) . array([[1. , 0.42121072], [0.42121072, 1. ]]) . Random Variable . Assigns a numerical value to the outcome of random experiment. . . Distributions . Histogram . Plots frequency of values against values. A histogram can tell following in data . . Probability Density Functions . Continuous | Discrete | . . Cumulative Density Function . The max on y axis for CDF will be 1 as all the probabilities will add upto 1. . . Conditional Probability . Measure of probability of an event occurring given that another event has occurred. . For dependent Events . $P( mathrm{A} | mathrm{B})= frac{ mathrm{P}( mathrm{A} cap mathrm{B})}{P( mathrm{B})}$ . $ mathrm{P}( mathrm{A} | mathrm{B})=$ Probability of $ mathrm{A}$, given $ mathrm{B}$ occurs . $ mathrm{P}( mathrm{A} cap mathrm{B})=$ Probability of $ mathrm{A}$ and $ mathrm{B}$ occurring . $ mathrm{P}( mathrm{B})=$ Probability of $ mathrm{B}$ . For Independent Events . $P(A | B)=P(A) quad$ (if $A$ and $B$ independent) . Lets see some example of coin tossings. . . Here is the tree diagram for combinations. . Finally if you sum up, all the combinations will result in sum of 1. . (1/4) + (1/4) + (1/4) + (1/4) = 1 . # Lets see all possible combinations of coin tossings from itertools import product tossings = set(product([&#39;H&#39;, &#39;T&#39;], repeat=3)) print(&quot;All possible combinations of coin 3 tossings&quot;) tossings . All possible combinations of coin 3 tossings . {(&#39;H&#39;, &#39;H&#39;, &#39;H&#39;), (&#39;H&#39;, &#39;H&#39;, &#39;T&#39;), (&#39;H&#39;, &#39;T&#39;, &#39;H&#39;), (&#39;H&#39;, &#39;T&#39;, &#39;T&#39;), (&#39;T&#39;, &#39;H&#39;, &#39;H&#39;), (&#39;T&#39;, &#39;H&#39;, &#39;T&#39;), (&#39;T&#39;, &#39;T&#39;, &#39;H&#39;), (&#39;T&#39;, &#39;T&#39;, &#39;T&#39;)} . # filter by 1st trail is Head first_head = {item for item in tossings if item[0] == &#39;H&#39;} first_head . {(&#39;H&#39;, &#39;H&#39;, &#39;H&#39;), (&#39;H&#39;, &#39;H&#39;, &#39;T&#39;), (&#39;H&#39;, &#39;T&#39;, &#39;H&#39;), (&#39;H&#39;, &#39;T&#39;, &#39;T&#39;)} . two_head = {item for item in tossings if item.count(&#39;H&#39;) == 2} two_head . {(&#39;H&#39;, &#39;H&#39;, &#39;T&#39;), (&#39;H&#39;, &#39;T&#39;, &#39;H&#39;), (&#39;T&#39;, &#39;H&#39;, &#39;H&#39;)} . # p(first_head / two_head) : probability of first one being head given there are 2 heads def probability(items): return len(items) / len(tossings) . def conditional_probability(A, B): return len(A &amp; B) / len(B) . probability(first_head) . 0.5 . probability(two_head) . 0.375 . conditional_probability(first_head, two_head) . 0.6666666666666666 . Central Limit Theorem . The distribution of mean of all the samples will be normal distribution even if actual population is not normal. . import numpy as np import pandas as pd from numpy.random import seed from numpy.random import randint from numpy import mean import seaborn as sns import matplotlib.pyplot as plt # seed the random number generator seed(1) # generate a sample of weights of population weights = randint(low=120, high=200, size=10000) print(&#39;The average weight is {} pounds&#39;.format(mean(weights))) weight_df = pd.DataFrame(data={&#39;weight_in_pounds&#39;: weights}) weight_df.head() . The average weight is 159.6552 pounds . /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . weight_in_pounds . 0 157 | . 1 132 | . 2 192 | . 3 129 | . 4 195 | . # Lets visualize the population weight frequency graph sns.distplot(weight_df[&#39;weight_in_pounds&#39;], color=&quot;blue&quot;) plt.xlabel(&quot;random variable of weights&quot;) plt.ylabel(&quot;probability of occurence&quot;) plt.title(&quot;Distribution of weight of people&quot;); . Lets collect the mean of all the samples taken from the population . no_of_samples_list = [20, 100, 1000] # total samples n total_mean_list = [] # to store mean of each sample caclulated mean_of_mean_list = [] for n in no_of_samples_list: mean_list_given_sample_num = [] for sample_no in range(n): curr_sample = np.random.choice(weight_df[&#39;weight_in_pounds&#39;], size = 100) # each sample size k mean = np.mean(curr_sample) mean_list_given_sample_num.append(mean) total_mean_list.append(mean_list_given_sample_num) mean_of_mean_list.append(np.mean(mean_list_given_sample_num)) # Lets view the distribution and frequency of mean of this samples # Make the graph 40 inches by 40 inches # fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5), sharex=True) # plot numbering starts at 1 plot_number=1 for mean_list in total_mean_list: ax = sns.distplot(mean_list, color=&quot;blue&quot;) print(&quot;plt number {} and mean of mean {}&quot;.format(plot_number, np.mean(mean_list))) ax.set_title(&quot;no of samples {}&quot;.format(len(mean_list))) # Go to the next plot for the next loop plot_number = plot_number + 1 plt.show() . plt number 1 and mean of mean 158.977 . plt number 2 and mean of mean 159.8567 . plt number 3 and mean of mean 159.63302000000002 . when the number of samples increased, the distribution of mean of samples tends to become normal distribution function. . Lets see the mean of means for different no of samples values. . mean_of_mean_list . [160.49999999999997, 159.65560000000002, 159.65964000000002] . Lets visualize all the 3 in single plot to compare. . sns.distplot(total_mean_list[0], label=&quot;mean of samples for $n={}$&quot;.format(no_of_samples_list[0])) sns.distplot(total_mean_list[1], label=&quot;mean of samples for $n={}$&quot;.format(no_of_samples_list[1])) sns.distplot(total_mean_list[2], label=&quot;mean of samples for $n={}$&quot;.format(no_of_samples_list[2])) plt.title(&quot;Distribution of Sample Means of People&#39;s Mass in Pounds&quot;, y=1.015, fontsize=20) plt.xlabel(&quot;sample mean mass [pounds]&quot;) plt.ylabel(&quot;frequency of occurence&quot;) plt.legend(); . References . https://dfrieds.com/math/central-limit-theorem.html .",
            "url": "https://sudheer.github.io/blog/machine%20learning/2020/05/20/_05_02_Statistics.html",
            "relUrl": "/machine%20learning/2020/05/20/_05_02_Statistics.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Logistic regression in python",
            "content": "Introduction . Logistic regression uses the logistic sigmoid function to return a probability value from feature variables. . How logistic regression works ? . . Examples . A person is obese or not ? | Does Mr A has cancer ? | Will this team win the match today ? | Email is spam or not ? | . . Why not linear regression . Linear regression predicts output as continuous range from $- infty$ to $+ infty$. But we are predicting discrete values like 0 and 1 in case of logistic regression. | Moreover we can&#39;t map all the output values onto a straight line as in case of linear function. There is huge chance that we miss predictions as shown in figure below | . . In logistic regression, the output of linear regression is passed to a sigmoid funtion to convert the predicted continuous to discrete categorical values. . Linear Regression . . Logistic Regression . . Let&#39;s see the differences between Linear and Logistic . $ begin{array}{l|l|l} hline &amp; text { Linear } &amp; text { Logistic } hline text { Target Variables } &amp; text { Continuous } &amp; text { Categorical } hline text { Problem Type } &amp; text { Regression } &amp; text { Classification } hline text { Hypothesis } &amp; theta^{T} x &amp; sigmoid left( theta^{T} x right) hline text { Loss } &amp; text { Mean Squared } &amp; text { Logistic } end{array}$ . Types . Binary: Output dependent variabels mapped to 2 categorical values | Multinomial: Three or more categorical values for classification | Ordinal: Three or more categorical values with ordering | . Math intro . . Odds and Log Odds . Since the goal of logistic function is to map linear combination of input variabels into a probability, we need a link to map linear combination to probability, and that link is logit function. Before knowing about logit functions, let&#39;s see what odds, log odds and odds ratio mean. . Odds . $ begin{aligned} operatorname{odds}(Y=1) &amp;= frac{P(Y=1)}{P(Y=0)}= frac{P(Y=1)}{1-P(Y=1)} &amp;= frac{p}{1-p} = frac{Probability of event happening}{Probability of event not happening} end{aligned}$ . Lets check the odds for a sample data . import pandas as pd data = [[&#39;CS&#39;, &#39;Dropout&#39;], [&#39;EE&#39;, &#39;Graduated&#39;], [&#39;CS&#39;, &#39;Dropout&#39;], [&#39;CS&#39;, &#39;Graduated&#39;], [&#39;EE&#39;, &#39;Dropout&#39;], [&#39;CS&#39;, &#39;Dropout&#39;], [&#39;CS&#39;, &#39;Dropout&#39;],[&#39;EE&#39;,&#39;Graduated&#39;]] df = pd.DataFrame(data, columns = [&#39;Branch&#39;, &#39;Status&#39;]) pd.crosstab(index=df[&#39;Branch&#39;], columns= df[&#39;Status&#39;], margins=True) . Status Dropout Graduated All . Branch . CS 4 | 1 | 5 | . EE 1 | 2 | 3 | . All 5 | 3 | 8 | . # odds of cs graduated odds_cs_grad = (1/5)/(4/5) # p/(1-p) print(&quot;odds of cs graduated {}&quot;.format(odds_cs)) . odds of cs graduated 0.25 . # odds of EE graduated odds_ee = (2/3)/(1/3) # p/(1-p) print(&quot;odds of ee graduated {}&quot;.format(odds_ee)) . odds of ee graduated 2.0 . # Odds ratio odds_ratio = odds_ee/odds_cs print(&quot;odds ratio of ee to cs is {}&quot;.format(odds_ratio)) print(&quot;A EE student is {} times likely to graduate than CS&quot;.format(odds_ratio)) . odds ratio of ee to cs is 8.0 A EE student is 8.0 times likely to graduate than CS . Lets plot log and log odds functions . import matplotlib.pyplot as plt import numpy as np %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; def odds(p): return p / (1 - p) def log_odds(p): return np.log(p / (1 - p)) x = np.arange(0.01, 1, 0.05) odds_x = odds(x) log_odds_x = log_odds(x) fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4)) plt.axvline(0) plt.axhline(0) axes[0].plot(x, odds_x) axes[0].set_title(&quot;odds function&quot;) axes[0].set(xlabel=&quot;x&quot;, ylabel=&quot;odds&quot;) axes[1].plot(x, log_odds_x) axes[1].set_title(&quot;log odds function&quot;) axes[1].set(xlabel=&quot;x&quot;, ylabel=&quot;log_odds&quot;) fig.tight_layout() . Logit function . $ operatorname{logit}(p)= log left( frac{P}{1-P} right), text { for } 0 leq p leq 1$ . This logit function is what we are trying to equate it to our linear combination of input variables. . $ log( frac{P}{1-P}) = theta_1 x_i + theta_0$ . $P = frac{1}{1+e^( theta_1 x_i + theta_0)}$ This exactly looks like sigmoid function which we will study below. . $P$ = probability of success . $- infty leq x_i leq infty$; . Sigmoid function . Sigmoid function is used in the logistic regression to map infinite values into a finite discrete target values. . Equation of sigmoid function is $g(z)= frac{1}{1+e^{-z}}$ . The function is plotted below . $ begin{aligned} &amp; lim _{x rightarrow infty} g(z)=1 &amp; lim _{x rightarrow- infty} g(z)=0 end{aligned}$ . Interesting thing about sigmoid function is, even the derivative of it can be expressed as the function itself. The first order derivate of sigmoid function is $ frac{d g(z)}{d z}=g(z)[1-g(z)]$ . import numpy as np import matplotlib.pyplot as plt %matplotlib inline z = np.linspace (-10,10,100) # sigmoid function def sigmoid(z): return 1 / (1 + np.exp(-z)) plt.figure(figsize=(10,6)) plt.plot(z,sigmoid(z)) plt.xlim([-10,10]) plt.ylim([-0.1,1.1]) plt.axvline(0) plt.axhline(0) plt.xlabel(&#39;z&#39;); plt.ylabel(&#39;g(z)&#39;) plt.title(&#39;Sigmoid function&#39;); plt. show ( ) . Bernoulli Distribution . We need to get some basics of Bernoulli Distribution here. Bernoulli says . $f_{ text {Bernoulli}}= left { begin{array}{ll} 1-P ; &amp; text { for } n=0 P ; quad text { for } n=1 end{array} right.$ . where $n = 0$ is failure event and $n = 1$ is a successful event. . Hypothesis . This equation takes the featurs (x) and parameters ($ theta$) as input and predicts the output dependent variable. . The weighted combination of input variables is ... $ theta_{1} cdot x_{1}+ theta_{2} cdot x_{2}+ ldots+ theta_{n} cdot x_{n}$ . Writing the above function in linear algebra from ... . $ sum_{i=1}^{m} theta_{i} x_{i}= theta^{T} x$ . Lets write this in matrix form . $ left[ begin{array}{c} theta_{1} theta_{2} cdot cdot theta_{n} end{array} right]^{T} cdot left[ begin{array}{c} x_{1} x_{2} cdot cdot x_{n} end{array} right]= left[ begin{array}{cccc} theta_{1} &amp; theta_{2} ldots theta_{n} end{array} right] cdot left[ begin{array}{c} x_{1} x_{2} cdot cdot cdot x_{n} end{array} right]= theta_{1} x_{1}+ theta_{2} x_{2}+ ldots+ theta_{n} x_{n}$ . If we pass this equation to sigmoid function .... . $P left( theta^{T} x right)=g left( theta^{T} x right) = frac{1}{1+e^{- theta^{T} x}}$ . where $P left( theta^{T} x right) = h_{ theta}(x)$ and $g()$ is called sigmoid function. . Now the hypothesis can be written as $h_{ theta}(x)= frac{1}{1+e^{- theta^{T} x}}$ . where $h_{ Theta}(x)=P(Y=1 | X ; theta )$ . In words Probability that $Y=1$ for features $X$ with co-efficients $ theta$ . Cost Function . we can&#39;t use the cost function Sum of Squared errors (SSE) in logistic regression as it would give convex graph and we will get lot of local minima and makes it very difficult to reach to a point of global minima. . . In linear regression, we have used Sum of squared errors (SSE) for calculating cost. In logistic regression we use slightly different approach. Suppose if a function predicts sucess % of 90 and seem to be a failure, we penalize it heavily than 30% probability prediction. . So for logistic regression, we go for a logarithemic cost function as below. The log cost function penalizes confident and wrong predictions heavily . $ operatorname{cost} left(h_{ theta}(x), y right)= left { begin{array}{ll} - log left(h_{ theta}(x) right) &amp; text { if } y=1 - log left(1-h_{ theta}(x) right) &amp; text { if } y=0 end{array} right.$ . if we convert the above to one liner ... . $ operatorname{cost} left(h_{ theta}(x), y right)=-y log left(h_{ theta}(x) right)-(1-y) log left(1-h_{ theta}(x) right)$ . Finally the cost function for all the values will be . $ begin{aligned} J( theta) &amp;= frac{1}{m} sum_{i=1}^{m} operatorname{cost} left(h_{ theta} left(x^{(i)} right), y^{(i)} right) &amp;=- frac{1}{m} left[ sum_{i=1}^{m} y^{(i)} log left(h_{ theta} left(x^{(i)} right) right)+ left(1-y^{(i)} right) log left(1-h_{ theta} left(x^{(i)} right) right) right] end{aligned}$ . Minimize cost function . use gradient descent to minimize the cost function . $ frac{ partial J( theta)}{ partial theta_{j}}= frac{1}{m} sum_{i=1}^{m} left(h left(x^{i} right)-y^{i} right) x_{j}^{i}$ . def costFunction(theta, X, y): &quot;&quot;&quot; Compute cost and gradient for logistic regression. Parameters - theta : weight vector of shape (n+1, ) X : input of shape (m x n+1) m: no of training ex, n:no of features y : predicted y (m, ). Returns - cost : value of cost function grad : vector of shape (n+1, ) -&gt; gradient of the cost fun wrt weights &quot;&quot;&quot; m = X.shape[0] # number of training examples # initialize Returns cost = 0 grads = np.zeros(theta.shape) #Prediction sigmoid_result = sigmoid(x.dot(theta)) Y_T = y.T cost = (-1/m)*(np.sum((Y_T*np.log(sigmoid_result)) + ((1-Y_T)*(np.log(1-sigmoid_result))))) # #Gradient calculation dw = (1/m)*(np.dot(X.T, (sigmoid_result-Y.T).T)) db = (1/m)*(np.sum(sigmoid_result-Y.T)) grads = {&quot;dw&quot;: dw, &quot;db&quot;: db} return cost, grads .",
            "url": "https://sudheer.github.io/blog/machine%20learning/2020/05/20/_04_2020_logistic_regression.html",
            "relUrl": "/machine%20learning/2020/05/20/_04_2020_logistic_regression.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Interactive and visualize linear regression in python",
            "content": ". Linear regression assumes relation between input variable and output. It predicts the output as a linear function from input variables. . Various methods of Linear Regression . Simple Linear Regression Requires to calculate mean, std-deviation, co-variance, corelation | . | Ordinary Least Squares uses Linear Algebra to estimate best values for co-efficients | . | Gradient Descent Iteratively miniize the error of model this it gets to a minimum Batch Gradient descent | Scochastic gradient descent | mini batch gradient descent | . | . | Regularized Linear Regression Lasso Regression | Ridge Regression | . | . Introduction and Terminology . Lets say we have below sample data. . size in feet price ($) . 2104 | 460 | . 1416 | 232 | . $m$ = number of training example . $x$ = input variable/ feature/ independent variable/ predictor . $y$ = output variable/ target variable/ dependent variable . . How do we represent h ? . Hypothesis: $h_ theta(x) = theta_0 + theta_1 * x$ . $h(x)$ is predictor of $y$ . This is linear regression with one variable . Let&#39;s express this as Matrix and vectors . $h_ theta(x) = begin{pmatrix} x 1 end{pmatrix}^T * begin{pmatrix} theta_1 theta_0 end{pmatrix}$ . Let $ X = begin{pmatrix} x 1 end{pmatrix}^T$ and $ theta = begin{pmatrix} theta_1 theta_0 end{pmatrix}$ . we can write it as $ h_ theta(x) = X * theta $ . We need to choose $ theta_0$ and $ theta_1$ so it best fits our function . Simple Linear Regression . uses stats to estimate co-efficients | need to calculate mean, std-devitation, co-variance and corelation. | . Lets generate some data for simple linear regression. Lets take house price prediction example . Linear equation for our calculations is $y = theta_1 * x + theta_0$ . Steps: . Estimate the slope $ theta_1$ | . begin{equation} theta_1 = frac{ sum(x_i – mean(x)) (y_i – mean(y)} { sum(x_i – mean(x))^2} end{equation}where . begin{equation} mean(x) = frac{1}{n} sum_{i=1}^n x_i end{equation} Estimate the intercept $ theta_0$ | . begin{equation} theta_0 = mean(y) – theta_1 * mean(x) end{equation} Estimating Error | . Root Mean Square Error . $RMSE = sqrt{( frac{1}{n}) sum_{i=1}^{n}(h_{i} - y_{i})^{2}}$ . $h_i$ is predicted value and $y_i$ is actual value . # imports import pandas as pd import numpy as np from matplotlib import pyplot as plt . Generate Data . # Generate &#39;random&#39; data np.random.seed(0) no_of_records=200 sq_ft = 50 * np.random.rand(no_of_records) + 500 # Array of 100 values with mean = 1000 res = 5 * np.random.randn(no_of_records) # Generate 100 residual terms price = 2 * sq_ft + 1000 + res # Actual values of Y # create pandas dataframe to store size and price df = pd.DataFrame( {&#39;sq_ft&#39;: sq_ft, &#39;price&#39;: price} ) # first five rows of our dataframe df.head() . sq_ft price . 0 527.440675 | 2060.514530 | . 1 535.759468 | 2066.119279 | . 2 530.138169 | 2054.538994 | . 3 527.244159 | 2052.299218 | . 4 521.182740 | 2039.875318 | . Visualize Data . X = df.iloc[:, 0] Y = df.iloc[:, 1] plt.title(&#39;House Price vs Sq ft&#39;) plt.xlabel(&#39;Sq ft&#39;) plt.ylabel(&#39;House Price&#39;) plt.scatter(X,Y) . &lt;matplotlib.collections.PathCollection at 0x7f80421cfa20&gt; . Build the model . Lets calculate . begin{equation} m = frac{ sum(x_i – mean(x)) (y_i – mean(y)} { sum(x_i – mean(x))^2} end{equation} begin{equation} c = mean(y) – theta_1 * mean(x) end{equation} # Building the model X_mean = np.mean(X) Y_mean = np.mean(Y) num = 0 den = 0 for i in range(len(X)): num += (X[i] - X_mean)*(Y[i] - Y_mean) # sum of all differences in numerator den += (X[i] - X_mean)**2 # sum of all differences in denom m = num / den # final slope theta_1 c = Y_mean - m*X_mean # final intercept theta_0 print (m, c) . 1.956195597832525 1022.4369105614524 . Now our linear equation is $House Price (Y) = (1.956195597832525) * x + (1022.4369105614524)$ . Lets plot and visualize it . m = 1.956195597832525 b = 1022.4369105614524 Y_pred = m*X+b # predicted values plt.scatter(X,Y) plt.plot(X,Y_pred,color=&#39;red&#39;) plt.show() . Check accuracy . Lets check how accurate is our model by estimating error. . $RMSE = sqrt{( frac{1}{n}) sum_{i=1}^{n}(h_{i} - y_{i})^{2}}$ . Y = df.iloc[:, 1] # observed Y_pred summation = 0 #summation of differences n = len(Y) #no of training examples for i in range (0,n): difference = Y[i] - Y_pred[i] #finding the difference between observed and predicted value squared_difference = difference**2 #square of the differene summation = summation + squared_difference #sum of all the differences MSE = summation/n #dividing summation by no of training examples print(&quot;ROOT Mean Square Error is: {}&quot;.format(MSE)) . ROOT Mean Square Error is: 22.826601317067663 . This mean 22% of time our estimation is not accurate. . Gradient Descent . Cost function &amp; Gradient Descents . Lets see how can we choose parameters $ theta_0$ and $ theta_1$ . Let y be result after substitution . Cost Function: . Loss Function $J( theta_0, theta_1) = sum limits_{i=1}^m (y_{predicted} - y_{actual})^2$ . MSE $J( theta_0, theta_1) = dfrac{1}{2m} sum limits_{i=1}^m (h_ theta(x^i) - y^i)^2$ . Goal : minimize $J( theta_0, theta_1)$ . Linear regression uses Gradient Descent to minimize the cost Function . Update rules begin{aligned} &amp; theta_{0}:= theta_{0}- alpha frac{ partial}{ partial theta_{0}} J left( theta_{0}, theta_{1} right) &amp; theta_{1}:= theta_{1}- alpha frac{ partial}{ partial theta_{1}} J left( theta_{0}, theta_{1} right) end{aligned} . Partial derivatives . begin{array}{l} frac{ partial}{ partial theta_{0}} J left( theta_{0}, theta_{1} right)= frac{1}{m} sum_{i=1}^{m} left(h_{ theta} left(x^{(i)} right)-y^{(i)} right) frac{ partial}{ partial theta_{1}} J left( theta_{0}, theta_{1} right)= frac{1}{m} sum_{i=1}^{m} left(h_{ theta} left(x^{(i)} right)-y^{(i)} right) cdot x^{(i)} end{array}Gradient Descent procedure . start with initial values for coefficients | . coefficient = 0.0 . calculate cost of coefficient | . cost = f(coefficient) . calculate derivative of cost function | . delta = derivative(cost) . update coefficients | . coefficient = coefficient - (alpha * delta) . This process is updated until cost function is minimum | . Batch Gradient descent . Pseudo Code . while True: Wgradient = evaluate_gradient(loss, data, W) W += -alpha * Wgradient . #populate x with input variable (sq_ft) and y with actual output variable (price) x = df[&#39;sq_ft&#39;] y = df[&#39;price&#39;] x = (x - x.mean()) / x.std() x = np.c_[np.ones(x.shape[0]), x] #Gradient Descent Algo learning_rate = 0.01 #learning rate (or) step_size iterations = 1000 #no of iterations m = y.size #no of training examples np.random.seed(123) #Set the seed theta = np.random.rand(2) #Pick some random values to start with def cal_cost(theta,x,y): m = len(y) predictions = x.dot(theta) cost = (1/2*m) * np.sum(np.square(predictions-y)) return cost def cal_co_efficients(theta, x, y): predictions = x.dot(theta) error = predictions-y theta = theta - (learning_rate * (1/m) * np.dot(x.T, error)) return theta #gradient descent function def gradient_descent(x, y, theta, iterations, learning_rate): prev_costs = [] prev_thetas = [theta] for i in range(iterations): # calculating predicted output y_predicted = m * x + c cost = cal_cost(theta, x, y) prev_costs.append(cost) theta = cal_co_efficients(theta, x, y) prev_thetas.append(theta) return prev_thetas, prev_costs #Pass the relevant variables to the function and get the new values back... prev_thetas, prev_costs = gradient_descent(x, y, theta, iterations, learning_rate) theta = prev_thetas[-1] #Print the results... print(&quot;Gradient Descent: {:.2f}, {:.2f}&quot;.format(theta[0], theta[1])) . Gradient Descent: 2049.39, 27.84 . #Plot the cost function... plt.title(&#39;Cost Function J&#39;) plt.xlabel(&#39;No. of iterations&#39;) plt.ylabel(&#39;Cost&#39;) plt.plot(past_costs) plt.show() . Stochastic Gradient descent pseudo code . [] . Pseudo Code . while True: batch = next_training_batch(data, 256) Wgradient = evaluate_gradient(loss, batch, W) W += -alpha * Wgradient . #populate x with input variable (sq_ft) and y with actual output variable (price) x = df[&#39;sq_ft&#39;] y = df[&#39;price&#39;] x = (x - x.mean()) / x.std() x = np.c_[np.ones(x.shape[0]), x] #Gradient Descent Algo learning_rate = 0.01 #learning rate (or) step_size iterations = 50 #no of iterations m = y.size #no of training examples np.random.seed(123) #Set the seed theta = np.random.rand(2) #Pick some random values to start with def cal_cost(theta,x,y): m = len(y) predictions = x.dot(theta) cost = (1/2*m) * np.sum(np.square(predictions-y)) return cost def cal_co_efficients(theta, x, y): predictions = x.dot(theta) error = predictions-y theta = theta - (learning_rate * (1/m) * np.dot(x.T, error)) return theta #gradient descent function def stocashtic_gradient_descent(x, y, theta, iterations, learning_rate): prev_costs = [] prev_thetas = [theta] cost =0.0 for i in range(iterations): rand_ind = np.random.randint(0,iterations) # calculating predicted output y_predicted = m * x + c cost = cal_cost(theta, x, y) prev_costs.append(cost) theta = cal_co_efficients(theta, x, y) prev_thetas.append(theta) return prev_thetas, prev_costs #Pass the relevant variables to the function and get the new values back... prev_thetas, prev_costs = stocashtic_gradient_descent(x, y, theta, iterations, learning_rate) theta = prev_thetas[-1] #Print the results... print(&quot;Stocashtic Gradient Descent: {:.2f}, {:.2f}&quot;.format(theta[0], theta[1])) . Stocashtic Gradient Descent: 809.95, 11.13 . #Plot the cost function... plt.title(&#39;Cost Function J&#39;) plt.xlabel(&#39;No. of iterations&#39;) plt.ylabel(&#39;Cost&#39;) plt.plot(prev_costs) plt.show() . Batch Gradient descent . #populate x with input variable (sq_ft) and y with actual output variable (price) x = df[&#39;sq_ft&#39;] y = df[&#39;price&#39;] x = (x - x.mean()) / x.std() #Gradient Descent Algo learning_rate = 0.01 #learning rate (or) step_size iterations = 10 #no of iterations batch_size = 20 m = y.size #no of training examples np.random.seed(123) #Set the seed theta = np.random.randn(2) #Pick some random values to start with def cal_cost(theta,x,y): m = len(y) predictions = x.dot(theta) cost = (1/2*m) * np.sum(np.square(predictions-y)) return cost def cal_co_efficients(theta, x, y): predictions = x.dot(theta) error = predictions-y theta = theta - (learning_rate * (1/m) * np.dot(x.T, error)) return theta #gradient descent function def minibatch_gradient_descent(x, y, theta, iterations, learning_rate, batch_size): m = len(y) n_batches = int(m/batch_size) prev_costs = [] prev_thetas = [theta] for i in range(iterations): cost =0.0 indices = np.random.permutation(m) x = x[indices] y = y[indices] for i in range(0,m,batch_size): x_i = x[i:i+batch_size] y_i = y[i:i+batch_size] x_i = np.c_[np.ones(len(x_i)),x_i] cost += cal_cost(theta,x_i,y_i) theta = cal_co_efficients(theta, x_i, y_i) prev_costs.append(cost) prev_thetas.append(theta) return prev_thetas, prev_costs #Pass the relevant variables to the function and get the new values back... prev_thetas, prev_costs = minibatch_gradient_descent(x, y, theta, iterations, learning_rate, batch_size) theta = prev_thetas[-1] #Print the results... print(&quot;Minibatch Gradient Descent: {:.2f}, {:.2f}&quot;.format(theta[0], theta[1])) . Minibatch Gradient Descent: 194.15, 3.54 . #Plot the cost function... plt.title(&#39;Cost Function J&#39;) plt.xlabel(&#39;No. of iterations&#39;) plt.ylabel(&#39;Cost&#39;) plt.plot(prev_costs) plt.show() .",
            "url": "https://sudheer.github.io/blog/ml/2020/05/20/_04_12_interactive_and_practical_linear_regression_using_python.html",
            "relUrl": "/ml/2020/05/20/_04_12_interactive_and_practical_linear_regression_using_python.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Linear Algebra for Machine Learning",
            "content": "import numpy %matplotlib inline from matplotlib import pyplot . Vector . Vector is a tuple of 1 or more scalars | . . # create a vector import numpy as np v = np.array([4,4]) print(v) v.shape . [4 4] . (2,) . # calculate magnitude of vector import math v = np.array([4,4]) vMag = math.sqrt(v[0]**2 + v[1]**2) print (vMag) . 5.656854249492381 . # calculate direction of vector v = np.array([4,4]) print (&#39;direction of vector v: &#39; + str(np.degrees(np.arctan2(v[1], v[0])))) . direction of vector v: 45.0 . # visualize vector vectors = [(4,4)] tails = [(-3,-2), (-3,1), (0,0), (1,-3)] plot_vector(vectors, tails) pyplot.title(&quot;The same vector, with its tail at four locations.&quot;); . Vector operations . $ begin{aligned} vec{A}+ vec{B} &amp;= left(a_{1}+b_{1}, a_{2}+b_{2}, a_{3}+b_{3} right) vec{A}- vec{B} &amp;= left(a_{1}-b_{1}, a_{2}-b_{2}, a_{3}-b_{3} right) alpha vec{A} &amp;= left( alpha a_{1}, alpha a_{2}, alpha a_{3} right) || vec{A}| | &amp;= sqrt{a_{1}^{2}+a_{2}^{2}+a_{3}^{2}} vec{A} cdot vec{B} &amp;=a_{1} b_{1}+a_{2} b_{2}+a_{3} b_{3} vec{A} times vec{B} &amp;= left(a_{2} b_{3}-a_{3} b_{2}, a_{3} b_{1}-a_{1} b_{3}, a_{1} b_{2}-a_{2} b_{1} right) end{aligned}$ . Dot and Cross product . Adding 2 vectors . $$ mathbf{a} = left[ begin{array}{c} -2 1 end{array} right], quad mathbf{b} = left[ begin{array}{c} 1 -3 end{array} right] $$ # vector addition a = numpy.array((-2,1)) b = numpy.array((1,-3)) origin = numpy.array((0,0)) vectors = [a, b] tails = [origin, origin] plot_vector(vectors, tails) pyplot.title(&quot;Initial 2 vecotrs with co-ordinates $(-2,1)$ and $(1,-3)$&quot;) plot_vector(vectors, [origin, a]) pyplot.title(&quot;2nd vector $(1,-3)$ moved to tip of 1st vector $(-2,1)$&quot;) plot_vector([a,b,a+b], [origin, a, origin]) pyplot.title(&quot;Adding vectors with coordinates $(-2, 1)$ and $(1,-3)$. n&quot;); . # multiply vectors from numpy import array a = array([4, 2, 3]) print(a) b = array([5, 2, 3]) print(b) c=a*b print(&quot;multiply vectors: {}&quot;.format(c)) . [4 2 3] [5 2 3] multiply vectors: [20 4 9] . Matrix . 2D array of scalars with one or more columns and one or more rows . $ begin{pmatrix} a &amp; b c &amp; d end{pmatrix}$ . Types of matrices . Creating a matrix . # create matrix from numpy import array A = array([[1, 0], [-3, 4]]) print(A) . [[ 1 0] [-3 4]] . Matrix Addition . # adding 2 matrices&quot; from numpy import array matrix1 = array([[1, 2, 3], [4, 5, 6]]) print(matrix1) matrix2 = array([[1, 2, 3], [4, 5, 6]]) print(matrix2) output = matrix1 + matrix2 print(&quot;adding 2 matrices: n {}&quot;.format(output)) . [[1 2 3] [4 5 6]] [[1 2 3] [4 5 6]] adding 2 matrices: [[ 2 4 6] [ 8 10 12]] . # Transpose of matrix from numpy import array # define square symmetric 3x3 matrix A = array([ [1, 2, 3], [2, 1, 2], [3, 2, 1] ]) print(A) # transpose A ATranspose = A.T print(&quot;transpose of matrix: n {}&quot;.format(ATranspose)) . [[1 2 3] [2 1 2] [3 2 1]] transpose of matrix: [[1 2 3] [2 1 2] [3 2 1]] . Matrix Transformation use case . # Image transformation from matplotlib import image from matplotlib import pyplot # load image as pixel array data = image.imread(&#39;/content/gdrive/My Drive/golden_gate.jpeg&#39;) # convert the image to 2D to represent as 2D matrix gray = data[:,:,0] # check the shape of the pixel array print(gray.dtype) print(gray.shape) # display the array of pixels as image pyplot.imshow(gray) pyplot.show() # transpose the matrix data_t = gray.T pyplot.imshow(data_t) pyplot.show() . uint8 (1797, 2732) . # inverse from numpy.linalg import inv A_I = inv(A) print(A_I) . [[-1.66666667 0.66666667] [ 1.33333333 -0.33333333]] . Dot product . # matrix dot product from numpy import array matrix1 = array([[1, 2], [3, 4], [5, 6]]) print(matrix1) matrix2 = array([[1, 2], [3, 4]]) print(matrix2) output = matrix1.dot(matrix2) print(&quot;dot product result: n {}&quot;.format(output)) . [[1 2] [3 4] [5 6]] [[1 2] [3 4]] dot product result: [[ 7 10] [15 22] [23 34]] . Dot product use case . Lets say we have house sizes and house price is a linear relation of house size. . $ {house Size} = 10190 + housePrice * 223$ . In traditional programming it will be like below . house_sizes = [630, 750, 790, 830, 900] house_prices = [] for i in house_sizes: house_price = 10190 + 223 * i house_prices.append(house_price) print(house_prices) . [150680, 177440, 186360, 195280, 210890] . How can we do this easily in linear algebra, can we represent as matrix multiplication ? . $ left[ begin{array}{cc} 1 &amp; 630 1 &amp; 750 1 &amp; 790 1 &amp; 830 1 &amp; 900 end{array} right] quad * quad left[ begin{array}{c} 10190 223 end{array} right]= left[ begin{array}{c} 150680 177440 186360 195280 210890 end{array} right]$ . import numpy as np matrix1 = np.array([[1,630],[1,750],[1,790],[1,830],[1,900]]) matrix2 = np.array([10190, 223]) matrix1.dot(matrix2) . array([150680, 177440, 186360, 195280, 210890]) . Matrix Decomposition . LU . QR . Eigen . Singular Value . Eigen Values and Eigen Vectors . $A X = lambda X$ . when a matrix transformation is applied to a vector and the resultant vector will only scale and not rotate, that vector is called eigen vector and scaling is called eigen value. . . Principle Component Analysis (PCA) . Singular Value Decomposition . References . https://minireference.com/static/tutorials/linear_algebra_in_4_pages.pdf . Essence of linear algebra .",
            "url": "https://sudheer.github.io/blog/machine%20learning/2020/05/20/Linear_algebra.html",
            "relUrl": "/machine%20learning/2020/05/20/Linear_algebra.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Kaggle Titanic dataset analysis",
            "content": ". Importing libraries . import pandas as pd import numpy as np import random as rnd import pylab as plot . !kaggle competitions download titanic -p ~/workspace/sudheer-blog/datasets/inputs/titanic/ . Downloading titanic.zip to /Users/sudheer/workspace/sudheer-blog/datasets/inputs/titanic 0%| | 0.00/34.1k [00:00&lt;?, ?B/s] 100%|██████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 7.00MB/s] . !unzip ~/workspace/sudheer-blog/datasets/inputs/titanic/titanic.zip -d ~/workspace/sudheer-blog/datasets/inputs/titanic &amp;&amp; rm ~/workspace/sudheer-blog/datasets/inputs/titanic/titanic.zip . Archive: /Users/sudheer/workspace/sudheer-blog/datasets/inputs/titanic/titanic.zip replace /Users/sudheer/workspace/sudheer-blog/datasets/inputs/titanic/gender_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C . import datasets . train_df = pd.read_csv(&#39;~/workspace/sudheer-blog/datasets/inputs/titanic/train.csv&#39;) test_df = pd.read_csv(&#39;~/workspace/sudheer-blog/datasets/inputs/titanic/test.csv&#39;) combine = [train_df, test_df] . Preview the datasets . train_df.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . test_df.head() . PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 892 | 3 | Kelly, Mr. James | male | 34.5 | 0 | 0 | 330911 | 7.8292 | NaN | Q | . 1 893 | 3 | Wilkes, Mrs. James (Ellen Needs) | female | 47.0 | 1 | 0 | 363272 | 7.0000 | NaN | S | . 2 894 | 2 | Myles, Mr. Thomas Francis | male | 62.0 | 0 | 0 | 240276 | 9.6875 | NaN | Q | . 3 895 | 3 | Wirz, Mr. Albert | male | 27.0 | 0 | 0 | 315154 | 8.6625 | NaN | S | . 4 896 | 3 | Hirvonen, Mrs. Alexander (Helga E Lindqvist) | female | 22.0 | 1 | 1 | 3101298 | 12.2875 | NaN | S | . Check the quality of the data . train_df.isnull().sum() . PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 . Both Age and Cabin has null lot of null values, lets see how much percentage of these are null . Check Null values percentage . train_df.isnull().sum(axis = 0)/train_df.count() . PassengerId 0.000000 Survived 0.000000 Pclass 0.000000 Name 0.000000 Sex 0.000000 Age 0.247899 SibSp 0.000000 Parch 0.000000 Ticket 0.000000 Fare 0.000000 Cabin 3.367647 Embarked 0.002250 Died 0.000000 dtype: float64 . Check Age distribution . train_df[&#39;Age&#39;].plot.hist(bins=12, alpha=0.5) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x115189710&gt; . Analyze by pivoting features . Lets analyse by following features . Age | Pclass | Sex | . Group by Age . sample = train_df sample[&#39;Died&#39;] = 1 - sample[&#39;Survived&#39;] sample.groupby(&#39;Sex&#39;).agg(&#39;sum&#39;)[[&#39;Survived&#39;,&#39;Died&#39;]].plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x114e36550&gt; . Group by Pclass . train_df[[&#39;Pclass&#39;, &#39;Survived&#39;]].groupby([&#39;Pclass&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Pclass Survived . 0 1 | 0.629630 | . 1 2 | 0.472826 | . 2 3 | 0.242363 | . Group by Sex . train_df[[&quot;Sex&quot;, &quot;Survived&quot;]].groupby([&#39;Sex&#39;], as_index=False).mean().sort_values(by=&#39;Survived&#39;, ascending=False) . Sex Survived . 0 female | 0.742038 | . 1 male | 0.188908 | .",
            "url": "https://sudheer.github.io/blog/notebooks/2020/01/17/titanic_kaggle.html",
            "relUrl": "/notebooks/2020/01/17/titanic_kaggle.html",
            "date": " • Jan 17, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Functional programming in scala",
            "content": "Introduction . Functions are first class values Functions can be defined with in other functions | Functions can be passed as params to other functions | Functions can be return types of functions | . | Immutability operations on objects creates new objects rather than modifying original | . | Pure functionsPure Function = O/p depends on I/p + No side effects . A pure function cannot rely on input from files, databases, web services etc | always produces same output with given input | . | . pure functiondef add(a:Int, b:Int):Int = { return a + b } . | impure functiondef getSysTime:Long = { return System.currentTimeMillis() } . | . Defining functions . def functionName(params : paramTypes) : functionReturnType = { // statements } . def sqrt(x: Double) = x * x . defined function sqrt . def area(a: Int = 0, b: Int = 0) = { a * b } area(2,3) area(a=2) . defined function area res5_1: Int = 6 res5_2: Int = 0 . Function evaluation strategies . def test() = { println(&quot;testing call by value/name&quot;) 1 // return value } . defined function test . call by value . def callByValue(x: Int) = { println(&quot;x1=&quot; + x) println(&quot;x2=&quot; + x) } callByValue(test()) . testing call by value/name x1=1 x2=1 . defined function callByValue . call by name . def callByName(x: =&gt; Int) = { println(&quot;x1=&quot; + x) println(&quot;x2=&quot; + x) } callByName(test()) . testing call by value/name x1=1 testing call by value/name x2=1 . defined function callByName . def vs val for functions . val add = (a: Int, b: Int) =&gt; a + b add(2,3) . def add(a: Int, b: Int) =&gt; a + b add(2,3) . Blocks in scala . println({ val a = 2 + 2 a + 1 }) . 5 . variable definitions in blocks . val varName:String= { val d = &quot;Hello World&quot; d.toString() } . varName: String = &#34;Hello World&#34; . def meth2():String = { val d = &quot;Hello World&quot; d.toString() } println(meth2) . Hello World . defined function meth2 . Higher order functions . take function as argument | return function as argument | . . def math(x: Double, y: Double, f: (Double, Double) =&gt; Double) : Double = f(x,y) math(10, 20, (a,b)=&gt;a+b) math(10,20, (a,b)=&gt;a-b) . defined function math res0_1: Double = 30.0 res0_2: Double = -10.0 . val list = List(1,2,3) list.map(x =&gt; x+1) // Higher order function passed to map . list: List[Int] = List(1, 2, 3) res10_1: List[Int] = List(2, 3, 4) . Partiallly applied Functions . function which may not be defined for all inputs | . Creating partial functions . val multiplyVal = (x: Int, y: Int) =&gt; x * y val multiplyCurried = multiplyVal.curried // The call to .curried transforms function value&#39;s type from (Int, Int) =&gt; Int to Int =&gt; (Int =&gt; Int) val partiallyAppliedFunc1 = multiplyCurried(2) val partiallyAppliedFunc2 = multiplyVal(2, _: Int) val res1 = partiallyAppliedFunc1(3) val res2 = partiallyAppliedFunc2(3) . multiplyVal: (Int, Int) =&gt; Int = ammonite.$sess.cmd27$Helper$$Lambda$2404/1209197431@12a200a0 multiplyCurried: Int =&gt; Int =&gt; Int = scala.Function2$$Lambda$2298/1567973346@219b84fc partiallyAppliedFunc1: Int =&gt; Int = scala.Function2$$Lambda$2308/1602704739@170ea369 partiallyAppliedFunc2: Int =&gt; Int = ammonite.$sess.cmd27$Helper$$Lambda$2405/294871968@3d7fc26c res1: Int = 6 res2: Int = 6 . Currying . . Pseudo Code . result = f(x)(y)(z) f1 = f(x) f2 = f1(y) result = f2(z) . def add(a: Int)(b: Int) = a + b val twoPlusOne = add(2)(1) val plusOneCounter = add(1) _ val result = plusOneCounter(2) println(s&quot;add(2)(1) output is ${twoPlusOne} and result is $result&quot;) . add(2)(1) output is 3 and result is 3 . defined function add twoPlusOne: Int = 3 plusOneCounter: Int =&gt; Int = ammonite.$sess.cmd15$Helper$$Lambda$2113/1951893763@55ae6ff8 result: Int = 3 . def add(a: Int, b: Int) = a + b val addCurried = add(2, _) sumCurried(6) . defined function sum sumCurried: Int =&gt; Int = ammonite.$sess.cmd32$Helper$$Lambda$2512/507796500@f24aa23 res32_2: Int = 8 . Functional Composition . val add1 = (a: Int) =&gt; a+1 val multiply3 = (a: Int) =&gt; a*3 . add1: Int =&gt; Int = ammonite.$sess.cmd1$Helper$$Lambda$1829/1144892518@2e7ed391 multiply3: Int =&gt; Int = ammonite.$sess.cmd1$Helper$$Lambda$1830/1298967826@3f84a3ec . // AndThen val andThenExample = add1 andThen multiply3 andThenExample(3) . andTahenExample: Int =&gt; Int = scala.Function1$$Lambda$320/899929247@27071089 res6_1: Int = 12 . // compose val composeExample = add1 compose multiply3 composeExample (1) . composeExample: Int =&gt; Int = scala.Function1$$Lambda$2020/929592323@3dec46e5 res9_1: Int = 4 . Declarative style . FP helps us to write declarative style of programming thus reducing the number of lines and defining a cleaner way. Given a list to modify, lets see both imperative vs declarative styles. . val x = List(10,20,30,40) . x: List[Int] = List(10, 20, 30, 40) . Imperative style . an apply method is generated | constructor params are public vals by default | equals and hashcode are generated | tostring method is generated | unapply method is generated | has built in copy method which is helpful in cloning | . import scala.collection.mutable.ListBuffer val mutable = new ListBuffer[Int] for (e &lt;- x) { mutable += (e * 3) } println(mutable.toList) . List(30, 60, 90, 120) . import scala.collection.mutable.ListBuffer mutable: ListBuffer[Int] = ListBuffer(30, 60, 90, 120) . Declarative style . val a = Seq({ val list = List(1,2,3,4) list.filter(x =&gt; x&gt;2) }) println(a) . List(List(3, 4)) . a: Seq[List[Int]] = List(List(3, 4)) . val result = x.map(element =&gt; element * 3) println(result) . List(30, 60, 90, 120) . result: List[Int] = List(30, 60, 90, 120) . Closures . Closures are functions whose return values depends on 1 or more values outside of function . var factor = 7 val multiplier = (i:Int) =&gt; i * factor multiplier(7) . factor: Int = 7 multiplier: Int =&gt; Int = ammonite.$sess.cmd31$Helper$$Lambda$2482/1360119030@25b7c499 res31_2: Int = 49 .",
            "url": "https://sudheer.github.io/blog/programming/2019/05/12/fp_scala.html",
            "relUrl": "/programming/2019/05/12/fp_scala.html",
            "date": " • May 12, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "Shortcuts and Commands",
            "content": ". Jupyter . Cell . m : Switch to markdown y : Switch to code Ctrl + Shift + - :while editing a cell should split it at the cursor D + D (press the key twice): delete current cell z : Undo cell deletion . Execute . Shift + Enter : execute and go to next cell Ctrl + Enter : execute and live in current cell . Insert . a : To create cell above to current cell b : To create cell bellow to current cell . Cut Copy Paste . x : To cut the cell that can be pasted any where c : To copy the cell that can be pasted any where v : To paste the cell . Markdown in jupyter . #, ## , ###, #### : headings **string** : bold text $ $ : math symbols &gt; :indenting- : bullets 1 : numbered bullets . Unix commands . !&lt;command&gt; . Download kaggle datasets . !pip install -q kaggle . !kaggle competitions --help .",
            "url": "https://sudheer.github.io/blog/installations/2018/12/03/shortcuts.html",
            "relUrl": "/installations/2018/12/03/shortcuts.html",
            "date": " • Dec 3, 2018"
        }
        
    
  
    
        ,"post10": {
            "title": "Pyspark quickstart",
            "content": "Creating RDD . # from list parallelRdd = sc.parallelize([1,2,3,4,5]) parallelRdd.collect() . &#9656; | : | . | . . [1, 2, 3, 4, 5] . # from tuple rdd = sc.parallelize((&#39;a&#39;,&#39;b&#39;,&#39;c&#39;)) rdd.collect() . &#9656; | : | . | . . [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] . # from list rdd = sc.parallelize([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;]) rdd.collect() . &#9656; | : | . | . . [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;] . # from set rdd = sc.parallelize({&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;}) rdd.collect() . &#9656; | : | . | . . [&#39;c&#39;, &#39;d&#39;, &#39;b&#39;, &#39;a&#39;] . # from dict rdd = sc.parallelize( { &#39;a&#39; : 1, &#39;b&#39; : 2, &#39;c&#39; : 3 }) rdd.collect() . &#9656; | : | . | . . [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] . As you see only keys are used in the dictionary to form RDD . # read from csv file rdd = sc.textFile(&#39;&#39;) rdd.take(2) . # create empty rdd rdd = sc.emptyRDD() rdd.isEmpty() . &#9656; | : | . | . . True . # from namedtuple from collections import namedtuple Person = namedtuple(&quot;Person&quot;, &quot;id firstName lastName&quot;) jon = Person(1, &quot;Jon&quot;, &quot;Doe&quot;) jane = Person(2, &quot;Jane&quot;, &quot;Doe&quot;) rdd = sc.parallelize([jon, jane]) rdd.collect() . &#9656; | : | . | . . [Person(id=1, firstName=&#39;Jon&#39;, lastName=&#39;Doe&#39;), Person(id=2, firstName=&#39;Jane&#39;, lastName=&#39;Doe&#39;)] . Test Rdd Functions . # Histogram rdd = sc.parallelize([1,2,3,4,5]) rdd.histogram([0,10][1]) . &#9656; | : | . | . . ([1.0, 1.4, 1.8, 2.2, 2.6, 3.0, 3.4000000000000004, 3.8000000000000003, 4.2, 4.6, 5], [1, 0, 1, 0, 0, 1, 0, 1, 0, 1]) . # sum rdd.sum() . &#9656; | : | . | . . 15 . rdd.distinct().count() . &#9656; | : | . | . . 2 . # convert rdd to pair rdd using map sc.parallelize([1,2,3,4,5,6]).map(lambda x: (x%3, x)).collect() . &#9656; | : | . | . . [(1, 1), (2, 2), (0, 3), (1, 4), (2, 5), (0, 6)] . # take sc.parallelize([1,2,3,4,5,6]).first() . &#9656; | : | . | . . 1 . # sort by key sc.parallelize([1,6,3,4,2,7]).map(lambda x: (x%3, x)).sortByKey(ascending=ascending, numPartitions=5).collect() DataFrame&#39;s . create dataframe by reading csv file . csvData = spark.read.csv( path=&#39;&#39;, sep=&#39;,&#39;, encoding=&#39;UTF-8&#39;, comment=None, header=True, inferSchema=True) csvData.show(n=5, truncate=False) . create dataframe by createDataFrame function . rdd = spark.createDataFrame( [ (1, &#39;foo&#39;), (2, &#39;bar&#39;), ], [&#39;id&#39;, &#39;txt&#39;] # schema of columns here ) rdd.show(2) . &#9656; | : | . | . . +++ | id|txt| +++ | 1|foo| | 2|bar| +++ . change column names . Option-1 . from pyspark.sql.functions import col rdd.selectExpr(&quot;id as emp_id&quot;, &quot;txt as name&quot;).printSchema() . &#9656; | : | . | . . root |-- emp_id: long (nullable = true) |-- name: string (nullable = true) . Option-2 . from pyspark.sql.functions import col rdd.select(col(&quot;id&quot;).alias(&quot;emp_id&quot;), col(&quot;txt&quot;).alias(&quot;name&quot;)).printSchema() . &#9656; | : | . | . . root |-- emp_id: long (nullable = true) |-- name: string (nullable = true) . Option-3 . rdd.registerTempTable(&quot;rdd&quot;) output = spark.sql(&quot;SELECT id AS emp_id, txt as Name from rdd&quot;) output.printSchema() . &#9656; | : | . | . . root |-- emp_id: long (nullable = true) |-- Name: string (nullable = true) . Group by and Aggregate . rdd = spark.createDataFrame( [ (&#39;GOOG&#39;, 1, 200000), (&#39;GOOG&#39;, 2, 150000), (&#39;AAPL&#39;, 3, 175000), (&#39;AAPL&#39;, 4, 180000) ], [&#39;company&#39;, &#39;emp_id&#39;, &#39;salary&#39;] # schema of columns here ) rdd.show() . &#9656; | : | . | . . +-+++ |company|emp_id|salary| +-+++ | GOOG| 1|200000| | GOOG| 2|150000| | AAPL| 3|175000| | AAPL| 4|180000| +-+++ . rdd.groupBy(&#39;company&#39;) . &#9656; | : | . | . . &lt;pyspark.sql.group.GroupedData at 0x7f351e56d278&gt; . rdd.groupBy(&#39;company&#39;).max().show() ## you see if gave max values for a company for both columns . &#9656; | : | . | . . +-+--+--+ |company|max(emp_id)|max(salary)| +-+--+--+ | AAPL| 4| 180000| | GOOG| 2| 200000| +-+--+--+ . rdd.groupBy(&#39;company&#39;).max(&#39;salary&#39;).show() . &#9656; | : | . | . . +-+--+ |company|max(salary)| +-+--+ | AAPL| 180000| | GOOG| 200000| +-+--+ . # Sum by Aggregate grouped_rdd = rdd.groupBy(&quot;company&quot;) grouped_rdd.agg({&#39;salary&#39;:&#39;avg&#39;}).show() . &#9656; | : | . | . . +-+--+ |company|avg(salary)| +-+--+ | AAPL| 177500.0| | GOOG| 175000.0| +-+--+ . rdd.groupBy(&quot;company&quot;) .count() .orderBy(&quot;count&quot;, ascending=False) .show(5) . &#9656; | : | . | . . +-+--+ |company|count| +-+--+ | AAPL| 2| | GOOG| 2| +-+--+ . Order by . rdd.orderBy(&#39;salary&#39;).show() . &#9656; | : | . | . . +-+++ |company|emp_id|salary| +-+++ | GOOG| 2|150000| | AAPL| 3|175000| | AAPL| 4|180000| | GOOG| 1|200000| +-+++ . rdd.orderBy(rdd[&quot;salary&quot;].desc()).show() . &#9656; | : | . | . . +-+++ |company|emp_id|salary| +-+++ | GOOG| 1|200000| | AAPL| 4|180000| | AAPL| 3|175000| | GOOG| 2|150000| +-+++ . Adding &amp; Dropping Columns . from pyspark.sql.types import DoubleType addCol = rdd.withColumn(&quot;doubleSalary&quot;, rdd[&#39;salary&#39;].cast(DoubleType())) addCol.printSchema() . &#9656; | : | . | . . root |-- company: string (nullable = true) |-- emp_id: long (nullable = true) |-- salary: long (nullable = true) |-- doubleSalary: double (nullable = true) . addCol.drop(&quot;doubleSalary&quot;).printSchema() . &#9656; | : | . | . . root |-- company: string (nullable = true) |-- emp_id: long (nullable = true) |-- salary: long (nullable = true) . Joins .",
            "url": "https://sudheer.github.io/blog/computing/2018/03/12/pyspark_quickstart.html",
            "relUrl": "/computing/2018/03/12/pyspark_quickstart.html",
            "date": " • Mar 12, 2018"
        }
        
    
  
    
        ,"post11": {
            "title": "Apach Spark Transformations",
            "content": ". Aggregate . The aggregate can return a different type than RDD on which we are working on. It allows users to apply 2 functions, one on top of each partition (input type T =&gt; U), other to aggregate the results of all the partitions into final result (merging 2 U&#39;s). Both the functions have to be commutative and associative. We can also specify a initial value. . val inputRdd = sc.parallelize(Array(1,2,3,4,5,6,7)) val output = inputRdd.aggregate(0)((x,y) =&gt; x+y, (u,v) =&gt; u+v); . inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:27 output: Int = 28 . cartesian . Return the Cartesian product of this RDD and another one. result contains all pairs of (a,b) where a belongs to rdd1 and b belongs to rdd2. . val rdd1 = sc.parallelize(1 to 2) val rdd2 = sc.parallelize(3 to 5) val output = rdd1.cartesian(rdd2).collect() . rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at &lt;console&gt;:25 rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at &lt;console&gt;:26 output: Array[(Int, Int)] = Array((1,3), (1,4), (1,5), (2,3), (2,4), (2,5)) . countByValue . Returns count of each unique value in this RDD as a local map of (value, count) pairs. Should be careful while using this when you have large data as it sends the results to driver. . val inputRdd = sc.parallelize(1 to 10) val output = inputRdd.countByValue() . inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at &lt;console&gt;:27 output: scala.collection.Map[Int,Long] = Map(5 -&gt; 1, 10 -&gt; 1, 1 -&gt; 1, 6 -&gt; 1, 9 -&gt; 1, 2 -&gt; 1, 7 -&gt; 1, 3 -&gt; 1, 8 -&gt; 1, 4 -&gt; 1) . collect . Used only when the rdd is small enough. Return an array that contains all of the elements in this RDD. . val output = sc.parallelize(1 to 10, 2).collect() . output: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) . distinct . Return a new RDD containing the distinct elements in this RDD. Shuffle happens on this transformation. . val data = Seq(1,1,2,2,3,4) val inputRdd = sc.parallelize(data) val output = inputRdd.distinct(); output.collect() . data: Seq[Int] = List(1, 1, 2, 2, 3, 4) inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:30 output: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at distinct at &lt;console&gt;:31 res0: Array[Int] = Array(1, 2, 3, 4) . filter . Return a new RDD containing only the elements that satisfy a predicate. . val data = Seq(1, 2, 3, 4) val inputRdd = sc.parallelize(data) val filterOutput = inputRdd.filter( s =&gt; s&gt;2 ) filterOutput.collect(); . data: Seq[Int] = List(1, 2, 3, 4) inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at &lt;console&gt;:29 filterOutput: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[15] at filter at &lt;console&gt;:30 res1: Array[Int] = Array(3, 4) . first . Return the first element in this RDD. . val output = sc.parallelize(1 to 10).first(); . output: Int = 1 . flatMap . Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. You need to supply (A ⇒ Iterable[B]) function to the flatMap i.e on each element of input A, Map is applied followed by flatten which is flatMap. . val data = Seq(1, 2, 3, 4) val inputRdd = sc.parallelize(data) val output = inputRdd.flatMap( s =&gt; List(s,s+1,s+2)); output.collect(); . data: Seq[Int] = List(1, 2, 3, 4) inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at &lt;console&gt;:31 output: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[18] at flatMap at &lt;console&gt;:32 res2: Array[Int] = Array(1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5, 6) . glom . Return an RDD created by coalescing all elements within each partition into an array. . val inputRdd = sc.parallelize(1 to 10, 2) val output = inputRdd.glom().collect(); . inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[19] at parallelize at &lt;console&gt;:27 output: Array[Array[Int]] = Array(Array(1, 2, 3, 4, 5), Array(6, 7, 8, 9, 10)) . groupBy . Return an RDD of grouped items. | Each group consists of a key and a sequence of elements mapping to that key. | Ordering is not guaranteed, not same for every execution. | Shuffle happens, better to use reduceby than groupby since it does not combine in each partition itself ( i.e groupBy happens in reduce phase), hence result high network traffic. | . val rdd1 = sc.parallelize(Array(2,3,4,1,3,4)) val output = rdd1.groupBy(x =&gt; x).collect() . rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[21] at parallelize at &lt;console&gt;:27 output: Array[(Int, Iterable[Int])] = Array((1,CompactBuffer(1)), (2,CompactBuffer(2)), (3,CompactBuffer(3, 3)), (4,CompactBuffer(4, 4))) . intersection . Return the intersection of this RDD and another one. | Result will not contain any duplicate. | Done by map, co-group, filter in the background. | performs a shuffle internally | . val output = sc.parallelize(Array(1,2,3,4)) .intersection(sc.parallelize(Array(3,4,5,6))) .collect() . output: Array[Int] = Array(3, 4) . map . Return a new RDD by applying a function to all elements of this RDD . val data = Seq(1, 2, 3, 4) val inputRdd = sc.parallelize(data) val output = inputRdd.map( s =&gt; s+1 ) // applying anonymus function to rdd elements. output.collect(); . data: Seq[Int] = List(1, 2, 3, 4) inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[32] at parallelize at &lt;console&gt;:32 output: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[33] at map at &lt;console&gt;:33 res3: Array[Int] = Array(2, 3, 4, 5) . mapPartitions . Return a new RDD by applying a function to each partition of this RDD. . val rdd1 = sc.parallelize(1 to 20,3) val output = rdd1.mapPartitions(x =&gt; x).collect(); . rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at &lt;console&gt;:27 output: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20) . mapPartitionsWithIndex . Similar to mapPartitions, additionally tracks the index of the original partition. . val rdd1 = sc.parallelize(1 to 20,3) val output = rdd1.mapPartitions(x =&gt; x).collect(); . rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[36] at parallelize at &lt;console&gt;:27 output: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20) . mapPartitionsWithIndex . Similar to mapPartitions, additionally tracks the index of the original partition. . val inputRdd = sc.parallelize(1 to 10, 2) val output = inputRdd.mapPartitionsWithIndex((idx, itr) =&gt; itr.map(s =&gt; (idx, s))).collect() . inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[38] at parallelize at &lt;console&gt;:27 output: Array[(Int, Int)] = Array((0,1), (0,2), (0,3), (0,4), (0,5), (1,6), (1,7), (1,8), (1,9), (1,10)) . randomSplit . Randomly splits this RDD with the provided weights. You can specify the fraction weights the output rdd&#39;s needs to split. However you can see they are not exactly equally split based on fraction as in example. . val inputRdd = sc.parallelize(1 to 10) val output = inputRdd.randomSplit(Array(0.5,0.5)) // return&#39;s array of rdd&#39;s output(0).collect() // rdd in 0th location output(1).collect() // rdd in 1st location . inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[40] at parallelize at &lt;console&gt;:28 output: Array[org.apache.spark.rdd.RDD[Int]] = Array(MapPartitionsRDD[41] at randomSplit at &lt;console&gt;:29, MapPartitionsRDD[42] at randomSplit at &lt;console&gt;:29) res4: Array[Int] = Array(2, 4, 7, 9) . reduce . Reduces the elements of this RDD. function in reduce obey&#39;s commutative and associative properties. . val output = sc.parallelize(1 to 5).reduce((u,v) =&gt; u + v) . output: Int = 15 . repartition . Return a new RDD that has exactly the passed argument partitions to this method. . val data = Seq(1, 2, 3, 4) val inputRdd = sc.parallelize(data,2) inputRdd.partitions; // Get the array of partitions of this RDD inputRdd.repartition(1) . data: Seq[Int] = List(1, 2, 3, 4) inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[44] at parallelize at &lt;console&gt;:30 res5: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[48] at repartition at &lt;console&gt;:34 . sample . Return a sampled subset of this RDD. . val inputRdd = sc.parallelize(1 to 10, 3) inputRdd.sample(true, 0.3, 0).collect() . inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[49] at parallelize at &lt;console&gt;:27 res6: Array[Int] = Array(9, 10) . sortBy . Return this RDD sorted by the given key function. You should pass function since its not pair rdd to generate key, boolean (asce/desc). . val output = sc.parallelize(Array(3,4,2,1)) .sortBy(x =&gt; x,false) // desc order by false .collect() . output: Array[Int] = Array(4, 3, 2, 1) . subtract . Subtracts elements of one rdd from other . val output = sc.parallelize(1 to 10).subtract(sc.parallelize(5 to 15)) output.collect() . output: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[62] at subtract at &lt;console&gt;:27 res7: Array[Int] = Array(1, 2, 3, 4) . take . Take the first num elements of the RDD. It works by first scanning one partition, and use the results from that partition to estimate the number of additional partitions needed to satisfy the limit. . val output = sc.parallelize(1 to 10).take(2) . output: Array[Int] = Array(1, 2) . takeSample . Return a fixed-size sampled subset of this RDD in an array. . sample takeSample . It returns an RDD | It returns an Array | . Return a fixed-size sampled subset | Return a fixed-size sampled subset | . Should specify sample as Double fraction arg | sample is specified as Int | . val inputRdd = sc.parallelize(1 to 10, 3) inputRdd.takeSample(true, 3, 0); . inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[64] at parallelize at &lt;console&gt;:27 res8: Array[Int] = Array(7, 7, 4) . toLocalIterator . Return an iterator by converting RDD into a scala iterator that contains all of the elements in this RDD. . val output = sc.parallelize(1 to 5, 1).toLocalIterator while (output.hasNext) { println(output.next) } . 1 2 3 4 5 . output: Iterator[Int] = empty iterator . union . Return the union of this RDD and another one. | Identical elements will appear multiple times. | Need to use distinct to eliminate them. | Can also use ++ instead of union. | . val a = sc.parallelize(1 to 10, 1) val b = sc.parallelize(10 to 20, 1) a.union(b).collect(); a.union(b).distinct().collect(); . a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[67] at parallelize at &lt;console&gt;:25 b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[68] at parallelize at &lt;console&gt;:26 res10: Array[Int] = Array(4, 16, 14, 6, 8, 12, 18, 20, 10, 2, 13, 19, 15, 11, 1, 17, 3, 7, 9, 5) .",
            "url": "https://sudheer.github.io/blog/computing/2017/01/12/spark_transformations.html",
            "relUrl": "/computing/2017/01/12/spark_transformations.html",
            "date": " • Jan 12, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "TBD .",
          "url": "https://sudheer.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sudheer.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}