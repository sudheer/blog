{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-12-2020-decision_trees.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNN0k9Yy2BK+uw3AsG+sKKS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnM6eDYgug0K",
        "colab_type": "text"
      },
      "source": [
        "# \"Decision Tree implementation in python\"\n",
        "> \"Decision Tree implementation in python\"\n",
        "\n",
        "- toc: true\n",
        "- comments: true\n",
        "- categories: [machine learning]\n",
        "- search_exclude: true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frLgblPHdW_w",
        "colab_type": "text"
      },
      "source": [
        "# Decision Tree\n",
        "\n",
        "Predictive model in the tree (Acyclic Graph) form that maps inputs to its target value from root to leaf.\n",
        "\n",
        "> Note: Greedy + Top down + Recursive partitioning\n",
        "\n",
        "**Example**\n",
        "\n",
        "[![](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggVERcblx0QVtwZXRhbF93aWR0aCA-IDEuNzVdIC0tPiB8bm98IEIoSXJpcyB2ZXJzaWNvbG9yKVxuXHRBIC0tPiB8eWVzfCBDKElyaXMgVmVyZ2luaWNhKVxuXG4iLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCJ9LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggVERcblx0QVtwZXRhbF93aWR0aCA-IDEuNzVdIC0tPiB8bm98IEIoSXJpcyB2ZXJzaWNvbG9yKVxuXHRBIC0tPiB8eWVzfCBDKElyaXMgVmVyZ2luaWNhKVxuXG4iLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCJ9LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)\n",
        "\n",
        "petal_width : feature\n",
        "Iris versicolor, Iris Verginica: labels\n",
        "\n",
        "# How do we construct tree\n",
        "\n",
        "![decision_tree](https://user-images.githubusercontent.com/8268939/82154837-74dbdf80-9825-11ea-8ddd-7f804e91774c.png)\n",
        "\n",
        "# when good choice ?\n",
        "- output is discrete\n",
        "- no large data\n",
        "- noise in data\n",
        "- categories or classes are disjoint\n",
        "\n",
        "# Types \n",
        "\n",
        "[![](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggVERcblx0QVtEZWNpc2lvbiBUcmVlXSAtLT4gQihDbGFzc2lmaWNhdGlvbiB0cmVlKVxuXHRBIC0tPiBDKFJlZ3Jlc3Npb24gdHJlZSlcblxuIiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifSwidXBkYXRlRWRpdG9yIjpmYWxzZX0)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggVERcblx0QVtEZWNpc2lvbiBUcmVlXSAtLT4gQihDbGFzc2lmaWNhdGlvbiB0cmVlKVxuXHRBIC0tPiBDKFJlZ3Jlc3Npb24gdHJlZSlcblxuIiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifSwidXBkYXRlRWRpdG9yIjpmYWxzZX0)\n",
        "\n",
        "Regression trees are used when dependent variable is continous. Classification trees are used when dependent variable is categorical.\n",
        "\n",
        "# Family of decision tree algo's\n",
        "\n",
        "- ID3\n",
        "- c4.5\n",
        "- c5.0\n",
        "- CART (uses Gini index to create splits)\n",
        "\n",
        "# Impurity functions\n",
        "\n",
        "Measure how pure a label set of leaf nodes.\n",
        "\n",
        "- Entropy based Measure\n",
        "- Gini Measure\n",
        "\n",
        "## Entropy\n",
        "\n",
        "Entropy is a measure of disorder in the dataset (or) how much varied the data is.\n",
        "\n",
        "Lets say we have dataset of N items .... $S=\\left \\{ \\left ( \\mathbf{x}_1,y_1 \\right ),\\dots,\\left ( \\mathbf{x}_n,y_n \\right ) \\right \\}$\n",
        "\n",
        "These N items should be divided into c categories or classes represented by $y_i$\n",
        "\n",
        "$y_i\\in\\left \\{ 1,\\dots,c \\right \\}$, where $c$ is the number of classes\n",
        "\n",
        "The entropy of our dataset is given by equation...\n",
        "\n",
        "$H=-\\sum_{i=1}^{C} p\\left(x_{i}\\right) \\log _{2} p\\left(x_{i}\\right)$\n",
        "\n",
        "where $p\\left(x_{i}\\right)$ = ratios of elements of each label $i$ in our dataset.\n",
        "\n",
        "$C$ = No of class items in our data.\n",
        "\n",
        "The value of entropy always lies between 0 to 1. 1 being worst and 0 being best. \n",
        "\n",
        "![OUgcx](https://user-images.githubusercontent.com/8268939/82160147-21c75400-9848-11ea-908c-a49284816886.png)\n",
        "\n",
        "Let's see with an example.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/8268939/82846025-c1659180-9e9b-11ea-90a7-23bef6fe0aba.png)\n",
        "\n",
        "\n",
        "## Information Gain(IG)\n",
        "\n",
        "Measure of decrease in disorder with the help of splitting the original dataset.\n",
        "\n",
        "In other words, its the difference between parent node impurity and weighted child node impurity\n",
        "\n",
        "Based on this value, we split the node and build decision tree.\n",
        "\n",
        "\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/8268939/82846481-e3f8aa00-9e9d-11ea-8c40-342e0fcdceb5.png)\n",
        "\n",
        "## Gini Impurity\n",
        "\n",
        "$G=\\sum_{i=1}^{C} p_i *(1-p_i)$ \n",
        "\n",
        "(or)\n",
        "\n",
        "$G = 1-\\sum_{i=1}^{C}\\left(p_{i}\\right)^{2}$\n",
        "\n",
        "where \n",
        "\n",
        "C : no of classes (or) target variables\n",
        "\n",
        "($p_i$) is the probability of class i in a node\n",
        "\n",
        "# Avoid overfitting Pruning & constrains\n",
        "\n",
        "- Technique that reduces size of tree by removing sub tree's that provide little value to classification\n",
        "- reduces overfitting chance\n",
        "- setting constrains on controlling depth\n",
        "  - max_depth\n",
        "  - min_samples_leaf: min num of samples a leaf can have to avoid furthur splitting\n",
        "  - max_leaf_nodes: limits total leaves in a tree\n",
        "\n",
        "# Comparision of decision tree algorithms\n",
        "\n",
        "![](https://d3i71xaburhd42.cloudfront.net/681bcde8e803deb939e03e9fdcae601ca0c5fff9/2-Table1-1.png)\n",
        "\n",
        "# How it works\n",
        "\n",
        "Lets see how one of the algorithm(ID3) which is used to calculate decision tree's work.\n",
        "\n",
        "[![](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggVERcblx0QVtTdGFydCB3aXRoIHJvb3RdIC0tPiBCKEZpbmQgZmVhdHVyZSB1c2VkIGFzIHJvb3Qgbm9kZSlcblx0QiAtLT4gQyh0ZXN0IGF0dHJpYnV0ZSlcbiAgICBDW0NhbCBpbmZvcm1hdGlvbiBHYWluXSAgLS0-IEQoU2VwYXJhdGUgRGF0YSBiYXNlZCBvbiBzZWxlY3RlZCBmZWF0dXJlKVxuICAgIEQgLS0-IENDe0NvbnN0cmFpbnMgc2F0aXNmaWVkID99XG4gICAgQ0MgLS0-IHxOb3wgQlxuICAgIENDIC0tPiB8WWVzfCBQKFBydW5pbmcpXG4gICAgUCAtLT4gRW5kXG5cbiIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In0sInVwZGF0ZUVkaXRvciI6ZmFsc2V9)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggVERcblx0QVtTdGFydCB3aXRoIHJvb3RdIC0tPiBCKEZpbmQgZmVhdHVyZSB1c2VkIGFzIHJvb3Qgbm9kZSlcblx0QiAtLT4gQyh0ZXN0IGF0dHJpYnV0ZSlcbiAgICBDW0NhbCBpbmZvcm1hdGlvbiBHYWluXSAgLS0-IEQoU2VwYXJhdGUgRGF0YSBiYXNlZCBvbiBzZWxlY3RlZCBmZWF0dXJlKVxuICAgIEQgLS0-IENDe0NvbnN0cmFpbnMgc2F0aXNmaWVkID99XG4gICAgQ0MgLS0-IHxOb3wgQlxuICAgIENDIC0tPiB8WWVzfCBQKFBydW5pbmcpXG4gICAgUCAtLT4gRW5kXG5cbiIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In0sInVwZGF0ZUVkaXRvciI6ZmFsc2V9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjefwcCLl86x",
        "colab_type": "text"
      },
      "source": [
        "# Implementation in python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llgoIDFWmAA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate entropy\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def entropy(y):\n",
        "  histo = np.bincount(y)\n",
        "  ps = histo/len(y)\n",
        "\n",
        "  entropy = -np.sum([p * np.log2(p) for p in ps if p > 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A9LO78ZnIzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define decision node\n",
        "class DecisionNode: \n",
        "  def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "    self.feature = feature\n",
        "    self.threshold = threshold\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "    self.value = value\n",
        "\n",
        "    def isLeafNode(self):\n",
        "      return self.value is not None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsXmhJ79n4wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decision tree class\n",
        "class DecisionTree:\n",
        "\n",
        "  def __init__(self, min_sample_split=2, max_depth=100, n_features=None):\n",
        "    self.min_sample_split = min_sample_split\n",
        "    self.max_depth = max_depth\n",
        "    self.n_features = n_features\n",
        "    self.root = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.root = self.grow_tree(X, y)\n",
        "\n",
        "  def grow_tree(self, X, y, depth=0):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_labels = len(np.unique(y))\n",
        "\n",
        "    # stopping criteria\n",
        "    if(depth >= self.max_depth or n_labels == 1 or n_samples < self.min_sample_split):\n",
        "      leaf_value = self.most_common_label(y)\n",
        "      return DecisionNode(value=leaf_value)\n",
        "\n",
        "    features_idxs = np.random.choice(n_features, self.n_features, replace=False)\n",
        "\n",
        "    # greedy search \n",
        "    best_feature, best_threshold = self.best_criteria(X, y , features_idxs)\n",
        "    left_idxs, right_idxs = self.split(X[:, best_feature], best_threshold)\n",
        "    left = self.grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "    right = self.grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "    return Node(best_feature, best_threshold, left, right)\n",
        "\n",
        "  def predict(self, X): \n",
        "    # traverse tree\n",
        "    return np.array([self._traverse_tree(x) for x in X], self.root)\n",
        "\n",
        "  def _traverse_tree(self, x, node):\n",
        "    if node.isLeafNode():\n",
        "      return node.value\n",
        "\n",
        "    if x[node.feature_idx] <= node.thresholds:\n",
        "      return self._traverse_tree(x, node.left)\n",
        "    return self._traverse_tree(x, node.right)\n",
        "\n",
        "  def most_common_label(self, y):\n",
        "    counter = Counter(y)\n",
        "    most_common = counter.most_common(1)[0]\n",
        "    return most_common\n",
        "\n",
        "  def best_criteria(self, X, y, feature_idxs):\n",
        "    best_gain = -1\n",
        "    split_idx, split_threshold = None, None\n",
        "    for feature_idx in feature_idxs:\n",
        "      X_column = X[:, feature_idx]\n",
        "      thresholds = np.unique(X_column)\n",
        "      for threshold in thresholds:\n",
        "        gain = self.information_gain(y, X_column, threshold)\n",
        "\n",
        "        if gain > best_gain:\n",
        "          best_gain = gain\n",
        "          split_idx = feature_idx\n",
        "          split_threshold = threshold\n",
        "    return split_idx, split_threshold\n",
        "\n",
        "  def information_gain(self, y, X_column, split_threshold):\n",
        "    parent_entropy = entropy(y)\n",
        "\n",
        "    # generate splits\n",
        "    left_idxs, right_idxs = self.split(X_column, split_threshold)\n",
        "\n",
        "    if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "      return 0\n",
        "\n",
        "    # weighted avg child E\n",
        "    n = len(y)\n",
        "    n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "    e_l, e_r = entropy(y[left_idxs], entropy[y[right_idxs]])\n",
        "    child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
        "\n",
        "    # return ig\n",
        "    ig = parent_entropy - child_entropy\n",
        "    return ig\n",
        "\n",
        "  def split(self, X_column, split_threshold):\n",
        "    left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n",
        "    right_idxs = np.argwhere(X_column > split_threshold).flatten()\n",
        "    return left_idxs, right_idxs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc2l9WPJmZkh",
        "colab_type": "text"
      },
      "source": [
        "# References \n",
        "\n",
        "- [U of waterloo](https://www.youtube.com/watch?v=E4HFVAjhQWQ&feature=youtu.be)\n",
        "\n",
        "- [Ytube](https://www.youtube.com/watch?v=Bqi7EFFvNOg&t=5s)\n",
        "\n",
        "- An Overview of Classification Algorithm in Data mining\n",
        "Sampath K. Kumar, Panneerselvam Kiruthika"
      ]
    }
  ]
}