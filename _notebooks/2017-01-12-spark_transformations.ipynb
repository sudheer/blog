{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "datastudio": {
      "id": "2awbzw8zgp2u",
      "kernel": "m1lwed4p1hdp"
    },
    "kernelspec": {
      "display_name": "spylon-kernel",
      "language": "scala",
      "name": "spylon-kernel"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "help_links": [
        {
          "text": "MetaKernel Magics",
          "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
        }
      ],
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala",
      "version": "0.4.1"
    },
    "colab": {
      "name": "Copy of 2017-01-12-spark_transformations.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7wUVOFh18DT",
        "colab_type": "text"
      },
      "source": [
        "# \"Apach Spark Transformations\"\n",
        "> \"Apach Spark Transformations\"\n",
        "\n",
        "- toc:true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [computing]\n",
        "- tags: [spark]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6lXDjXO18DU",
        "colab_type": "text"
      },
      "source": [
        "![Apache_Spark_logo svg](https://user-images.githubusercontent.com/8268939/79700662-3c5bdc80-824c-11ea-85aa-4559e7028a8d.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVWtdm-D18DV",
        "colab_type": "text"
      },
      "source": [
        "## Aggregate\n",
        "\n",
        "The aggregate can return a different type than RDD on which we are working on. It allows users to apply 2 functions, one on top of each partition (input type T => U), other to aggregate the results of all the partitions into final result (merging 2 U's). Both the functions have to be commutative and associative. We can also specify a initial value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-26T16:54:08.596073Z",
          "start_time": "2020-03-26T16:54:07.752Z"
        },
        "id": "ItEfewLu18DW",
        "colab_type": "code",
        "outputId": "23f7e229-ea00-4527-b283-18cf7243ccc4",
        "colab": {}
      },
      "source": [
        "val inputRdd = sc.parallelize(Array(1,2,3,4,5,6,7))\n",
        "val output = inputRdd.aggregate(0)((x,y) => x+y, (u,v) => u+v);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:27\n",
              "output: Int = 28\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zERL02-18Dc",
        "colab_type": "text"
      },
      "source": [
        "## cartesian\n",
        "\n",
        "Return the Cartesian product of this RDD and another one. result contains all pairs of (a,b) where a belongs to rdd1 and b belongs to rdd2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXZ1aSqc18De",
        "colab_type": "code",
        "outputId": "2e784c7e-8fc9-4a97-e322-76a3bda14667",
        "colab": {}
      },
      "source": [
        "val rdd1 = sc.parallelize(1 to 2)\n",
        "val rdd2 = sc.parallelize(3 to 5)\n",
        "val output = rdd1.cartesian(rdd2).collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:25\n",
              "rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[3] at parallelize at <console>:26\n",
              "output: Array[(Int, Int)] = Array((1,3), (1,4), (1,5), (2,3), (2,4), (2,5))\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "468Rf3-e18Dh",
        "colab_type": "text"
      },
      "source": [
        "## countByValue\n",
        "\n",
        "Returns count of each unique value in this RDD as a local map of (value, count) pairs. Should be careful while using this when you have large data as it sends the results to driver."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-26T16:54:01.080913Z",
          "start_time": "2020-03-26T16:53:59.967Z"
        },
        "id": "M9AV6ETR18Di",
        "colab_type": "code",
        "outputId": "5bd4f760-228c-4ff7-9c34-f71feb16d411",
        "colab": {}
      },
      "source": [
        "val inputRdd = sc.parallelize(1 to 10)\n",
        "val output = inputRdd.countByValue()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at <console>:27\n",
              "output: scala.collection.Map[Int,Long] = Map(5 -> 1, 10 -> 1, 1 -> 1, 6 -> 1, 9 -> 1, 2 -> 1, 7 -> 1, 3 -> 1, 8 -> 1, 4 -> 1)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHb4Y0ck18Dl",
        "colab_type": "text"
      },
      "source": [
        "## collect\n",
        "\n",
        "Used only when the rdd is small enough. Return an array that contains all of the elements in this RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFPwDHCr18Dl",
        "colab_type": "code",
        "outputId": "c302fdd7-f0c2-4e75-81e7-1410e80101ea",
        "colab": {}
      },
      "source": [
        "val output = sc.parallelize(1 to 10, 2).collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "output: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irkG7gvJ18Do",
        "colab_type": "text"
      },
      "source": [
        "## distinct\n",
        "\n",
        "Return a new RDD containing the distinct elements in this RDD. Shuffle happens on this transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4Vs5ABd18Do",
        "colab_type": "code",
        "outputId": "15663463-9cea-4552-95a9-df3a11d96937",
        "colab": {}
      },
      "source": [
        "val data = Seq(1,1,2,2,3,4)\n",
        "val inputRdd = sc.parallelize(data)\n",
        "val output = inputRdd.distinct();\n",
        "output.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "data: Seq[Int] = List(1, 1, 2, 2, 3, 4)\n",
              "inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at <console>:30\n",
              "output: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at distinct at <console>:31\n",
              "res0: Array[Int] = Array(1, 2, 3, 4)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PboTlfCp18Dq",
        "colab_type": "text"
      },
      "source": [
        "## filter\n",
        "\n",
        "Return a new RDD containing only the elements that satisfy a predicate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q0UqwDO18Dq",
        "colab_type": "code",
        "outputId": "894654ea-0e04-4c6a-f3c0-9790fccc180d",
        "colab": {}
      },
      "source": [
        "val data = Seq(1, 2, 3, 4)\n",
        "val inputRdd = sc.parallelize(data)\n",
        "val filterOutput = inputRdd.filter( s => s>2 )\n",
        "filterOutput.collect();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "data: Seq[Int] = List(1, 2, 3, 4)\n",
              "inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[14] at parallelize at <console>:29\n",
              "filterOutput: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[15] at filter at <console>:30\n",
              "res1: Array[Int] = Array(3, 4)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVWc9-f718Dr",
        "colab_type": "text"
      },
      "source": [
        "## first\n",
        "\n",
        "Return the first element in this RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44_vjCSc18Ds",
        "colab_type": "code",
        "outputId": "7bcc602f-bfa8-4880-ac38-f8b4f41d0ab8",
        "colab": {}
      },
      "source": [
        "val output = sc.parallelize(1 to 10).first();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "output: Int = 1\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itqO2Rut18Dt",
        "colab_type": "text"
      },
      "source": [
        "## flatMap\n",
        "\n",
        "Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. You need to supply (A ⇒ Iterable[B]) function to the flatMap i.e on each element of input A, Map is applied followed by flatten which is flatMap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-26T16:54:34.164387Z",
          "start_time": "2020-03-26T16:54:32.965Z"
        },
        "id": "DOxhS3e618Dt",
        "colab_type": "code",
        "outputId": "15442e71-e83c-4de6-ec28-bae8fd8a8c11",
        "colab": {}
      },
      "source": [
        "val data = Seq(1, 2, 3, 4)\n",
        "val inputRdd = sc.parallelize(data)\n",
        "val output = inputRdd.flatMap( s => List(s,s+1,s+2));\n",
        "output.collect();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "data: Seq[Int] = List(1, 2, 3, 4)\n",
              "inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at <console>:31\n",
              "output: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[18] at flatMap at <console>:32\n",
              "res2: Array[Int] = Array(1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5, 6)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsOP3dyS18Du",
        "colab_type": "text"
      },
      "source": [
        "## glom\n",
        "\n",
        "Return an RDD created by coalescing all elements within each partition into an array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtNfKcZX18Du",
        "colab_type": "code",
        "outputId": "26857d6f-3c84-42ba-e487-71d26ed60208",
        "colab": {}
      },
      "source": [
        "val inputRdd = sc.parallelize(1 to 10, 2)\n",
        "val output = inputRdd.glom().collect();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[19] at parallelize at <console>:27\n",
              "output: Array[Array[Int]] = Array(Array(1, 2, 3, 4, 5), Array(6, 7, 8, 9, 10))\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJrZi6f518Dw",
        "colab_type": "text"
      },
      "source": [
        "## groupBy\n",
        "\n",
        "- Return an RDD of grouped items.\n",
        "- Each group consists of a key and a sequence of elements mapping to that key.\n",
        "- Ordering is not guaranteed, not same for every execution.\n",
        "- Shuffle happens, better to use reduceby than groupby since it does not combine in each partition itself ( i.e groupBy happens in reduce phase), hence result high network traffic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2mxY95K18Dw",
        "colab_type": "code",
        "outputId": "e98ce433-15a7-4325-dd08-e900ef0705af",
        "colab": {}
      },
      "source": [
        "val rdd1 = sc.parallelize(Array(2,3,4,1,3,4))\n",
        "val output = rdd1.groupBy(x => x).collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[21] at parallelize at <console>:27\n",
              "output: Array[(Int, Iterable[Int])] = Array((1,CompactBuffer(1)), (2,CompactBuffer(2)), (3,CompactBuffer(3, 3)), (4,CompactBuffer(4, 4)))\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDNfH0Ru18Dx",
        "colab_type": "text"
      },
      "source": [
        "## intersection\n",
        "\n",
        "- Return the intersection of this RDD and another one.\n",
        "- Result will not contain any duplicate.\n",
        "- Done by map, co-group, filter in the background.\n",
        "- performs a shuffle internally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hAnAf-c18Dx",
        "colab_type": "code",
        "outputId": "597f8e3e-4864-494f-b3f5-af7fb0df8850",
        "colab": {}
      },
      "source": [
        "val output = sc.parallelize(Array(1,2,3,4))\n",
        "               .intersection(sc.parallelize(Array(3,4,5,6)))\n",
        "               .collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "output: Array[Int] = Array(3, 4)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wI-U35o18Dy",
        "colab_type": "text"
      },
      "source": [
        "## map\n",
        "\n",
        "Return a new RDD by applying a function to all elements of this RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tcd_e55Y18Dz",
        "colab_type": "code",
        "outputId": "3b997dc4-b2d7-4e3c-de9b-1cd5a21c0816",
        "colab": {}
      },
      "source": [
        "val data = Seq(1, 2, 3, 4)\n",
        "val inputRdd = sc.parallelize(data)\n",
        "val output = inputRdd.map( s => s+1 ) // applying anonymus function to rdd elements.\n",
        "output.collect();  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "data: Seq[Int] = List(1, 2, 3, 4)\n",
              "inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[32] at parallelize at <console>:32\n",
              "output: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[33] at map at <console>:33\n",
              "res3: Array[Int] = Array(2, 3, 4, 5)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD_lmSsi18D0",
        "colab_type": "text"
      },
      "source": [
        "## mapPartitions\n",
        "\n",
        "Return a new RDD by applying a function to each partition of this RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4UXoaD818D0",
        "colab_type": "code",
        "outputId": "c219f02a-e490-46f1-94ee-a5c9afa31188",
        "colab": {}
      },
      "source": [
        "val rdd1 = sc.parallelize(1 to 20,3)\n",
        "val output = rdd1.mapPartitions(x => x).collect();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at <console>:27\n",
              "output: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKtfVW9Q18D2",
        "colab_type": "text"
      },
      "source": [
        "## mapPartitionsWithIndex\n",
        "\n",
        "Similar to mapPartitions, additionally tracks the index of the original partition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYBBWS9J18D2",
        "colab_type": "code",
        "outputId": "3f0b2ae1-913a-4162-d9a2-dda81922cc56",
        "colab": {}
      },
      "source": [
        "val inputRdd = sc.parallelize(1 to 10, 2)\n",
        "val output = inputRdd.mapPartitionsWithIndex((idx, itr) => itr.map(s => (idx, s))).collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[38] at parallelize at <console>:27\n",
              "output: Array[(Int, Int)] = Array((0,1), (0,2), (0,3), (0,4), (0,5), (1,6), (1,7), (1,8), (1,9), (1,10))\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJKUcNzI18D3",
        "colab_type": "text"
      },
      "source": [
        "## randomSplit\n",
        "\n",
        "Randomly splits this RDD with the provided weights. You can specify the fraction weights the output rdd's needs to split. However you can see they are not exactly equally split based on fraction as in example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDoEqHol18D3",
        "colab_type": "code",
        "outputId": "5464921d-4b89-435a-eba0-c4fab8fc18a4",
        "colab": {}
      },
      "source": [
        "val inputRdd = sc.parallelize(1 to 10) \n",
        "val output = inputRdd.randomSplit(Array(0.5,0.5)) // return's array of rdd's   \n",
        "output(0).collect() // rdd in 0th location \n",
        "output(1).collect() // rdd in 1st location "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[40] at parallelize at <console>:28\n",
              "output: Array[org.apache.spark.rdd.RDD[Int]] = Array(MapPartitionsRDD[41] at randomSplit at <console>:29, MapPartitionsRDD[42] at randomSplit at <console>:29)\n",
              "res4: Array[Int] = Array(2, 4, 7, 9)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w90OEB2Z18D5",
        "colab_type": "text"
      },
      "source": [
        "## reduce\n",
        "\n",
        "Reduces the elements of this RDD. function in reduce obey's commutative and  associative properties."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbZ-TrQL18D5",
        "colab_type": "code",
        "outputId": "b7eac960-8aa9-44a4-ce5b-8675c37390a1",
        "colab": {}
      },
      "source": [
        "val output = sc.parallelize(1 to 5).reduce((u,v) => u + v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "output: Int = 15\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdvf1u6T18D6",
        "colab_type": "text"
      },
      "source": [
        "## repartition\n",
        "\n",
        "Return a new RDD that has exactly the passed argument partitions to this method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8T0FFxP18D6",
        "colab_type": "code",
        "outputId": "79a9c2b1-08e8-45b9-9442-51714fd670e1",
        "colab": {}
      },
      "source": [
        "val data = Seq(1, 2, 3, 4) \n",
        "val inputRdd = sc.parallelize(data,2) \n",
        "inputRdd.partitions; // Get the array of partitions of this RDD \n",
        "inputRdd.repartition(1) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "data: Seq[Int] = List(1, 2, 3, 4)\n",
              "inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[44] at parallelize at <console>:30\n",
              "res5: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[48] at repartition at <console>:34\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USAVl2c_18D7",
        "colab_type": "text"
      },
      "source": [
        "## sample\n",
        "\n",
        "Return a sampled subset of this RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADzOHxCE18D7",
        "colab_type": "code",
        "outputId": "36d0408d-5de8-4097-ff9b-23fcbf37d5b5",
        "colab": {}
      },
      "source": [
        "val inputRdd = sc.parallelize(1 to 10, 3) \n",
        "\n",
        "inputRdd.sample(true, 0.3, 0).collect()   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[49] at parallelize at <console>:27\n",
              "res6: Array[Int] = Array(9, 10)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tkInNrv18D9",
        "colab_type": "text"
      },
      "source": [
        "## sortBy\n",
        "\n",
        "Return this RDD sorted by the given key function. You should pass function since its not pair rdd to generate key, boolean (asce/desc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4UiDywL18D9",
        "colab_type": "code",
        "outputId": "00f13a3b-6fc8-408e-de09-ae6efba680c9",
        "colab": {}
      },
      "source": [
        "val output  = sc.parallelize(Array(3,4,2,1))\n",
        "                .sortBy(x => x,false) // desc order by false\n",
        "                .collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "output: Array[Int] = Array(4, 3, 2, 1)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dKHLX8V18D-",
        "colab_type": "text"
      },
      "source": [
        "## subtract\n",
        "\n",
        "Subtracts elements of one rdd from other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5EqZLmE18D-",
        "colab_type": "code",
        "outputId": "b58888af-565b-4a9b-c045-cc8b34c0609d",
        "colab": {}
      },
      "source": [
        "val output = sc.parallelize(1 to 10).subtract(sc.parallelize(5 to 15))\n",
        "output.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "output: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[62] at subtract at <console>:27\n",
              "res7: Array[Int] = Array(1, 2, 3, 4)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2snK6My18D_",
        "colab_type": "text"
      },
      "source": [
        "## take\n",
        "\n",
        "Take the first num elements of the RDD. It works by first scanning one partition, and use the results from that partition to estimate the number of additional partitions needed to satisfy the limit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exU3dtHA18D_",
        "colab_type": "code",
        "outputId": "5589c78b-e1b4-4b0d-cfc2-c6eb6fdb0488",
        "colab": {}
      },
      "source": [
        "val output = sc.parallelize(1 to 10).take(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "output: Array[Int] = Array(1, 2)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJrDWNU-18EA",
        "colab_type": "text"
      },
      "source": [
        "## takeSample\n",
        "\n",
        "Return a fixed-size sampled subset of this RDD in an array.\n",
        "\n",
        "sample | takeSample\n",
        "-------|-----------\n",
        "It returns an RDD | It returns an Array\n",
        "Return a fixed-size sampled subset\t| Return a fixed-size sampled subset\n",
        "Should specify sample as Double fraction arg | sample is specified as Int"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GHSGSj218EA",
        "colab_type": "code",
        "outputId": "2bec9186-8fcf-451d-cfd6-16d11d1e18e9",
        "colab": {}
      },
      "source": [
        "val inputRdd = sc.parallelize(1 to 10, 3) \n",
        "inputRdd.takeSample(true, 3, 0);   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inputRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[64] at parallelize at <console>:27\n",
              "res8: Array[Int] = Array(7, 7, 4)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8k0_fRe18EB",
        "colab_type": "text"
      },
      "source": [
        "## toLocalIterator\n",
        "\n",
        "Return an iterator by converting RDD into a scala iterator that contains all of the elements in this RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCOIe7oa18EB",
        "colab_type": "code",
        "outputId": "786c012f-4a81-4fd5-adbf-5d96a97be40a",
        "colab": {}
      },
      "source": [
        "val output = sc.parallelize(1 to 5, 1).toLocalIterator\n",
        "  \n",
        "  while (output.hasNext) {\n",
        "    println(output.next)\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "output: Iterator[Int] = empty iterator\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7JLAvhO18EC",
        "colab_type": "text"
      },
      "source": [
        "## union\n",
        "\n",
        "- Return the union of this RDD and another one.\n",
        "- Identical elements will appear multiple times.\n",
        "- Need to use distinct to eliminate them.\n",
        "- Can also use ++ instead of union."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1kAeY6I18ED",
        "colab_type": "code",
        "outputId": "4c2460b9-ce42-4339-e8f2-92631a1438ef",
        "colab": {}
      },
      "source": [
        "val a = sc.parallelize(1 to 10, 1)\n",
        "val b = sc.parallelize(10 to 20, 1)\n",
        "a.union(b).collect();\n",
        "    \n",
        "a.union(b).distinct().collect();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[67] at parallelize at <console>:25\n",
              "b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[68] at parallelize at <console>:26\n",
              "res10: Array[Int] = Array(4, 16, 14, 6, 8, 12, 18, 20, 10, 2, 13, 19, 15, 11, 1, 17, 3, 7, 9, 5)\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}
