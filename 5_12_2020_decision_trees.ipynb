{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-12-2020-decision_trees.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKPJkdVJ1XvbURl+yE0939"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnM6eDYgug0K",
        "colab_type": "text"
      },
      "source": [
        "# \"Decision Tree implementation in python\"\n",
        "> \"Decision Tree implementation in python\"\n",
        "\n",
        "- toc: true\n",
        "- comments: true\n",
        "- categories: [machine learning]\n",
        "- search_exclude: true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frLgblPHdW_w",
        "colab_type": "text"
      },
      "source": [
        "# Decision Tree\n",
        "\n",
        "Predictive model in the tree (Acyclic Graph) form that maps inputs to its target value from root to leaf.\n",
        "\n",
        "**Example**\n",
        "\n",
        "[![](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggVERcblx0QVtwZXRhbF93aWR0aCA-IDEuNzVdIC0tPiB8bm98IEIoSXJpcyB2ZXJzaWNvbG9yKVxuXHRBIC0tPiB8eWVzfCBDKElyaXMgVmVyZ2luaWNhKVxuXG4iLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCJ9LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggVERcblx0QVtwZXRhbF93aWR0aCA-IDEuNzVdIC0tPiB8bm98IEIoSXJpcyB2ZXJzaWNvbG9yKVxuXHRBIC0tPiB8eWVzfCBDKElyaXMgVmVyZ2luaWNhKVxuXG4iLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCJ9LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)\n",
        "\n",
        "# How do we construct tree\n",
        "\n",
        "![decision_tree](https://user-images.githubusercontent.com/8268939/82154837-74dbdf80-9825-11ea-8ddd-7f804e91774c.png)\n",
        "\n",
        "# when good choice ?\n",
        "- output is discrete\n",
        "- no large data\n",
        "- noise in data\n",
        "- categories or classes are disjoint\n",
        "\n",
        "# Types \n",
        "\n",
        "[![](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggVERcblx0QVtEZWNpc2lvbiBUcmVlXSAtLT4gQihDbGFzc2lmaWNhdGlvbiB0cmVlKVxuXHRBIC0tPiBDKFJlZ3Jlc3Npb24gdHJlZSlcblxuIiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifSwidXBkYXRlRWRpdG9yIjpmYWxzZX0)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggVERcblx0QVtEZWNpc2lvbiBUcmVlXSAtLT4gQihDbGFzc2lmaWNhdGlvbiB0cmVlKVxuXHRBIC0tPiBDKFJlZ3Jlc3Npb24gdHJlZSlcblxuIiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifSwidXBkYXRlRWRpdG9yIjpmYWxzZX0)\n",
        "\n",
        "Regression trees are used when dependent variable is continous. Classification trees are used when dependent variable is categorical.\n",
        "\n",
        "# Family of decision tree algo's\n",
        "\n",
        "- ID3\n",
        "- c4.5\n",
        "- c5.0\n",
        "- CART (uses Gini index to create splits)\n",
        "\n",
        "**Impurity function**\n",
        "Measure how pure a label set of leaf nodes.\n",
        "\n",
        "**Entropy**\n",
        "\n",
        "Entropy helps to get to leaf node quickly by helping to select the next right feature variable in our training data. It measures the purity of split.\n",
        "\n",
        "$H(X)=-\\sum_{i=1}^{N} p\\left(x_{i}\\right) \\log _{2} p\\left(x_{i}\\right)$\n",
        "\n",
        "where $N$ = cardinality of our random variable X.\n",
        "\n",
        "For a training set containing $p$ positive examples and $n$ negetive examples...\n",
        "\n",
        "$H\\left(\\frac{p}{p+n}, \\frac{n}{p+n}\\right)=-\\frac{p}{p+n} \\log _{2} \\frac{p}{p+n}-\\frac{n}{p+n} \\log _{2} \\frac{n}{p+n}$\n",
        "\n",
        "The value of entropy always lies between 0 to 1. 1 being worst and 0 being best. \n",
        "\n",
        "![OUgcx](https://user-images.githubusercontent.com/8268939/82160147-21c75400-9848-11ea-908c-a49284816886.png)\n",
        "\n",
        "\n",
        "**Information Gain(IG)**\n",
        "\n",
        "Difference between parent node impurity and weighted child node impurity\n",
        "\n",
        "Based on this value, we split the node and build decision tree.\n",
        "\n",
        "$IG(\\underbrace{f}_{\\text {feature split-point }}, \\underbrace{s p}_{N})=I(\\text { parent })-\\left(\\frac{N_{\\text {left }}}{N} I(\\text { left })+\\frac{N_{\\text {right }}}{N} I(\\text { right })\\right)$\n",
        "\n",
        "\n",
        "#overfitting\n",
        "\n",
        "![](https://user-images.githubusercontent.com/8268939/82167675-67e4dd80-9871-11ea-84cc-7a54d04886f3.png)\n",
        "\n",
        "## Avoid overfitting Pruning & constrains\n",
        "\n",
        "- Technique that reduces size of tree by removing sub tree's that provide little value to classification\n",
        "- reduces overfitting chance\n",
        "- setting constrains on controlling depth\n",
        "\n",
        "# How it works\n",
        "\n",
        "[![](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggVERcblx0QVtTdGFydCB3aXRoIHJvb3RdIC0tPiBCKEZpbmQgZmVhdHVyZSB1c2VkIGFzIHJvb3Qgbm9kZSlcblx0QiAtLT4gQyh0ZXN0IGF0dHJpYnV0ZSlcbiAgICBDW0NhbCBpbmZvcm1hdGlvbiBHYWluXSAgLS0-IEQoU2VwYXJhdGUgRGF0YSBiYXNlZCBvbiBzZWxlY3RlZCBmZWF0dXJlKVxuICAgIEQgLS0-IENDe0NvbnN0cmFpbnMgc2F0aXNmaWVkID99XG4gICAgQ0MgLS0-IHxOb3wgQlxuICAgIENDIC0tPiB8WWVzfCBQKFBydW5pbmcpXG4gICAgUCAtLT4gRW5kXG5cbiIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In0sInVwZGF0ZUVkaXRvciI6ZmFsc2V9)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggVERcblx0QVtTdGFydCB3aXRoIHJvb3RdIC0tPiBCKEZpbmQgZmVhdHVyZSB1c2VkIGFzIHJvb3Qgbm9kZSlcblx0QiAtLT4gQyh0ZXN0IGF0dHJpYnV0ZSlcbiAgICBDW0NhbCBpbmZvcm1hdGlvbiBHYWluXSAgLS0-IEQoU2VwYXJhdGUgRGF0YSBiYXNlZCBvbiBzZWxlY3RlZCBmZWF0dXJlKVxuICAgIEQgLS0-IENDe0NvbnN0cmFpbnMgc2F0aXNmaWVkID99XG4gICAgQ0MgLS0-IHxOb3wgQlxuICAgIENDIC0tPiB8WWVzfCBQKFBydW5pbmcpXG4gICAgUCAtLT4gRW5kXG5cbiIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In0sInVwZGF0ZUVkaXRvciI6ZmFsc2V9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjefwcCLl86x",
        "colab_type": "text"
      },
      "source": [
        "# Implementation in python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llgoIDFWmAA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate entropy\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def entropy(y):\n",
        "  histo = np.bincount(y)\n",
        "  ps = histo/len(y)\n",
        "\n",
        "  entropy = -np.sum([p * np.log2(p) for p in ps if p > 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A9LO78ZnIzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define decision node\n",
        "class DecisionNode: \n",
        "  def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
        "    self.feature = feature\n",
        "    self.threshold = threshold\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "    self.value = value\n",
        "\n",
        "    def isLeafNode(self):\n",
        "      return self.value is not None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsXmhJ79n4wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decision tree class\n",
        "class DecisionTree:\n",
        "\n",
        "  def __init__(self, min_sample_split=2, max_depth=100, n_features=None):\n",
        "    self.min_sample_split = min_sample_split\n",
        "    self.max_depth = max_depth\n",
        "    self.n_features = n_features\n",
        "    self.root = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.root = self.grow_tree(X, y)\n",
        "\n",
        "  def grow_tree(self, X, y, depth=0):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_labels = len(np.unique(y))\n",
        "\n",
        "    # stopping criteria\n",
        "    if(depth >= self.max_depth or n_labels == 1 or n_samples < self.min_sample_split):\n",
        "      leaf_value = self.most_common_label(y)\n",
        "      return DecisionNode(value=leaf_value)\n",
        "\n",
        "    features_idxs = np.random.choice(n_features, self.n_features, replace=False)\n",
        "\n",
        "    # greedy search \n",
        "    best_feature, best_threshold = self.best_criteria(X, y , features_idxs)\n",
        "    left_idxs, right_idxs = self.split(X[:, best_feature], best_threshold)\n",
        "    left = self.grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "    right = self.grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "    return Node(best_feature, best_threshold, left, right)\n",
        "\n",
        "  def predict(self, X): \n",
        "    # traverse tree\n",
        "    return np.array([self._traverse_tree(x) for x in X], self.root)\n",
        "\n",
        "  def _traverse_tree(self, x, node):\n",
        "    if node.isLeafNode():\n",
        "      return node.value\n",
        "\n",
        "    if x[node.feature_idx] <= node.thresholds:\n",
        "      return self._traverse_tree(x, node.left)\n",
        "    return self._traverse_tree(x, node.right)\n",
        "\n",
        "  def most_common_label(self, y):\n",
        "    counter = Counter(y)\n",
        "    most_common = counter.most_common(1)[0]\n",
        "    return most_common\n",
        "\n",
        "  def best_criteria(self, X, y, feature_idxs):\n",
        "    best_gain = -1\n",
        "    split_idx, split_threshold = None, None\n",
        "    for feature_idx in feature_idxs:\n",
        "      X_column = X[:, feature_idx]\n",
        "      thresholds = np.unique(X_column)\n",
        "      for threshold in thresholds:\n",
        "        gain = self.information_gain(y, X_column, threshold)\n",
        "\n",
        "        if gain > best_gain:\n",
        "          best_gain = gain\n",
        "          split_idx = feature_idx\n",
        "          split_threshold = threshold\n",
        "    return split_idx, split_threshold\n",
        "\n",
        "  def information_gain(self, y, X_column, split_threshold):\n",
        "    parent_entropy = entropy(y)\n",
        "\n",
        "    # generate splits\n",
        "    left_idxs, right_idxs = self.split(X_column, split_threshold)\n",
        "\n",
        "    if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "      return 0\n",
        "\n",
        "    # weighted avg child E\n",
        "    n = len(y)\n",
        "    n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "    e_l, e_r = entropy(y[left_idxs], entropy[y[right_idxs]])\n",
        "    child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
        "\n",
        "    # return ig\n",
        "    ig = parent_entropy - child_entropy\n",
        "    return ig\n",
        "\n",
        "  def split(self, X_column, split_threshold):\n",
        "    left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n",
        "    right_idxs = np.argwhere(X_column > split_threshold).flatten()\n",
        "    return left_idxs, right_idxs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc2l9WPJmZkh",
        "colab_type": "text"
      },
      "source": [
        "# References \n",
        "\n",
        "[U of waterloo](https://www.youtube.com/watch?v=E4HFVAjhQWQ&feature=youtu.be)\n",
        "\n",
        "[Ytube](https://www.youtube.com/watch?v=Bqi7EFFvNOg&t=5s)"
      ]
    }
  ]
}